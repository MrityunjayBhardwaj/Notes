<!DOCTYPE html>
<html data-rh="lang" class="gr__towardsdatascience_com" lang="en"><link type="text/css" id="dark-mode" rel="stylesheet" href=""><style type="text/css" id="dark-mode-custom-style"></style><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/branch-latest.js"></script><script async="" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Audio AI: isolating vocals from stereo music using Convolutional Neural Networks</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-11-12T02:16:28.151Z"><meta data-rh="true" name="title" content="Audio AI: isolating vocals from stereo music using Convolutional Neural Networks"><meta data-rh="true" property="og:title" content="Audio AI: isolating vocals from stereo music using Convolutional Neural Networks"><meta data-rh="true" property="twitter:title" content="Audio AI: isolating vocals from stereo music using Convolutional Neural Networks"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/210532383785"><meta data-rh="true" property="al:android:url" content="medium://p/210532383785"><meta data-rh="true" property="al:ios:url" content="medium://p/210532383785"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="What if we could go back to 1965, knock on Abbey Road Studios’ front door holding an ‘All Access’ badge, and have the privilege of listening to those signature Lennon-McCartney harmonies A-Capella…"><meta data-rh="true" property="og:description" content="disclaimer: all intellectual property and techniques described in this article have been previously disclosed in US Patents US10014002B2…"><meta data-rh="true" property="twitter:description" content="disclaimer: all intellectual property and techniques described in this article have been previously disclosed in US Patents US10014002B2…"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*TWabaG5N1WaKuALbr8lGHw.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*TWabaG5N1WaKuALbr8lGHw.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@ale.koretzky"><meta data-rh="true" name="author" content="Ale Koretzky"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="15 min read"><meta data-rh="true" name="parsely-post-id" content="210532383785"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@ale.koretzky"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/210532383785"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="535" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@-moz-keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@-webkit-keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@-moz-keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@-webkit-keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}@-moz-keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}@keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}@-webkit-keyframes k6{100%{stroke-dashoffset:50.24px}}@-moz-keyframes k6{100%{stroke-dashoffset:50.24px}}@keyframes k6{100%{stroke-dashoffset:50.24px}}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:absolute}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ag{max-width:1192px}.ah{min-width:0}.ai{width:100%}.aj{height:65px}.am{flex:1 0 auto}.an{flex:0 0 auto}.ao{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ap{font-style:normal}.aq{line-height:20px}.ar{font-size:15.8px}.as{letter-spacing:0px}.at{color:rgba(0, 0, 0, 0.54)}.au{fill:rgba(0, 0, 0, 0.54)}.av{justify-content:flex-end}.aw{margin-top:16px}.ax{margin-bottom:16px}.ay{display:inherit}.az{max-width:210px}.ba{text-overflow:ellipsis}.bb{overflow:hidden}.bc{white-space:nowrap}.bd{display:inline-block}.be{border:none}.bf{outline:none}.bg{font:inherit}.bh{font-size:16px}.bi{opacity:0}.bj{position:relative}.bk{width:0px}.bl{transition:width 140ms ease-in}.bm{color:inherit}.bn{fill:inherit}.bo{font-size:inherit}.bp{border:inherit}.bq{font-family:inherit}.br{letter-spacing:inherit}.bs{font-weight:inherit}.bt{padding:0}.bu{margin:0}.bv:hover{cursor:pointer}.bw:hover{color:rgba(0, 0, 0, 0.9)}.bx:hover{fill:rgba(0, 0, 0, 0.9)}.by:focus{outline:none}.bz:disabled{cursor:default}.ca:disabled{color:rgba(0, 0, 0, 0.54)}.cb:disabled{fill:rgba(0, 0, 0, 0.54)}.cc{margin-left:16px}.cd{margin-right:10px}.cg{display:none}.ci{margin-right:16px}.cj{margin:15px 0}.ck{visibility:hidden}.cl{padding:4px 12px}.cm{color:rgba(0, 0, 0, 0.84)}.cn{background:0}.co{border-color:rgba(0, 0, 0, 0.54)}.cp:hover{color:rgba(0, 0, 0, 0.97)}.cq:hover{fill:rgba(0, 0, 0, 0.97)}.cr:hover{border-color:rgba(0, 0, 0, 0.84)}.cs:disabled{fill:rgba(0, 0, 0, 0.76)}.ct:disabled{border-color:rgba(0, 0, 0, 0.2)}.cu:disabled{cursor:inherit}.cv:disabled:hover{color:rgba(0, 0, 0, 0.54)}.cw:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.cx:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.cy{border-radius:4px}.cz{border-width:1px}.da{border-style:solid}.db{box-sizing:border-box}.dc{text-decoration:none}.dd{padding-bottom:10px}.de{padding-top:10px}.df{border-radius:50%}.dg{height:32px}.dh{width:32px}.di{border-top:none}.dj{background-color:rgba(53, 88, 118, 1)}.dl{height:54px}.dm{margin-right:40px}.dn{height:36px}.do{width:100px}.dp{overflow:auto}.dq{flex:0 1 auto}.dr{list-style-type:none}.ds{line-height:40px}.dt{overflow-x:auto}.du{align-items:flex-start}.dv{margin-top:20px}.dw{padding-top:20px}.dx{height:80px}.dy{height:20px}.dz{margin-right:15px}.ea{margin-left:15px}.eb:first-child{margin-left:0}.ec{min-width:1px}.ed{background-color:rgba(197, 210, 225, 1)}.ee{font-weight:300}.ef{font-size:15px}.eg{color:rgba(197, 210, 225, 1)}.eh{text-transform:uppercase}.ei{letter-spacing:1px}.ej:hover{color:rgba(251, 255, 255, 1)}.ek:hover{fill:rgba(233, 241, 250, 1)}.el:disabled{color:rgba(150, 171, 191, 1)}.em:disabled{fill:rgba(150, 171, 191, 1)}.en{margin-bottom:0px}.eo{height:119px}.er{padding-left:24px}.es{padding-right:24px}.et{margin-left:auto}.eu{margin-right:auto}.ev{max-width:728px}.ew{flex-direction:column}.ex{top:calc(100vh + 100px)}.ey{bottom:calc(100vh + 100px)}.ez{width:10px}.fa{pointer-events:none}.fb{word-break:break-word}.fc{word-wrap:break-word}.fd:after{display:block}.fe:after{content:""}.ff:after{clear:both}.fg{clear:both}.fh{margin-top:0px}.fi{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.fj{cursor:zoom-in}.fk{z-index:auto}.fl{transition:opacity 100ms 400ms}.fm{height:100%}.fn{will-change:transform}.fo{transform:translateZ(0)}.fp{margin:auto}.fq{background-color:rgba(0, 0, 0, 0.05)}.fr{padding-bottom:32.129742962056305%}.fs{filter:blur(20px)}.ft{transform:scale(1.1)}.fu{visibility:visible}.fv{background:rgba(255, 255, 255, 1)}.fw{max-width:680px}.fx{line-height:1.23}.fy{letter-spacing:0}.fz{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.gk{margin-bottom:-0.27em}.gq{line-height:1.394}.hb{margin-bottom:-0.42em}.hh{margin-top:32px}.hi{justify-content:space-between}.hm{width:48px}.hn{height:48px}.ho{fill:rgba(3, 168, 124, 1)}.hp{flex-direction:row}.hq{width:calc(100% + 25px)}.hr{height:calc(100% + 25px)}.hs{top:50%}.ht{left:50%}.hu{transform:translateX(-50%) translateY(-50%)}.hv{margin-left:12px}.hw{margin-bottom:2px}.hy{max-height:20px}.hz{display:-webkit-box}.ia{-webkit-line-clamp:1}.ib{-webkit-box-orient:vertical}.ic:hover{text-decoration:underline}.id{margin-left:8px}.ie{padding:0px 8px}.if{color:rgba(90, 118, 144, 1)}.ig{fill:rgba(102, 138, 170, 1)}.ih{border-color:rgba(102, 138, 170, 1)}.ii:hover{color:rgba(84, 108, 131, 1)}.ij:hover{fill:rgba(90, 118, 144, 1)}.ik:hover{border-color:rgba(90, 118, 144, 1)}.il{line-height:18px}.im{align-items:flex-end}.iu{margin-right:8px}.iv{fill:rgba(0, 0, 0, 0.76)}.iw{margin-right:-6px}.ix{line-height:1.58}.iy{letter-spacing:-0.004em}.iz{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.ji{margin-bottom:-0.46em}.jj{font-style:italic}.jx{padding-bottom:40.989583333333336%}.jy{background-repeat:repeat-x}.jz{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.ka{background-size:1px 1px}.kb{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.kc{font-weight:700}.kd{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.ke{font-size:28px}.kf{margin-top:30px}.kg{text-align:center}.kh:before{content:"..."}.ki:before{letter-spacing:0.6em}.kj:before{text-indent:0.6em}.kk:before{font-style:italic}.kl:before{line-height:1.4}.km{padding-left:30px}.kn{line-height:1.48}.ko{letter-spacing:-0.014em}.kp{color:rgba(0, 0, 0, 0.76)}.kq{font-size:24px}.kw{font-size:30px}.kx{line-height:44px}.ky{line-height:1.12}.kz{letter-spacing:-0.022em}.la{font-weight:600}.lj{margin-bottom:-0.28em}.lp{line-height:1.4}.lq{margin-top:10px}.lt{padding-bottom:51.40625%}.lu{max-width:2862px}.lv{padding-bottom:5px}.lw{padding-top:5px}.lx{padding-bottom:50.733752620545076%}.ly{list-style-type:disc}.lz{margin-left:30px}.ma{padding-left:0px}.ml{list-style-type:decimal}.mm{padding-bottom:57.42857142857142%}.mn{max-width:1805px}.mo{padding-bottom:80.05540166204986%}.mp{padding-bottom:44.21875%}.mq{padding-bottom:51.82291666666667%}.mr{max-width:1070px}.ms{padding-bottom:100%}.mt{max-width:1706px}.mu{padding-bottom:58.14771395076202%}.mv{line-height:1.18}.ng{margin-bottom:-0.31em}.nh{max-width:1296px}.ni{padding-bottom:52.77777777777778%}.nj{padding-bottom:56.591099916036946%}.nk{padding:20px}.nl{background:rgba(0, 0, 0, 0.05)}.nm{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.nn{margin-top:-0.09em}.no{margin-bottom:-0.09em}.np{white-space:pre-wrap}.nv{max-width:535px}.nw{padding-bottom:356.4485981308411%}.nx{max-width:1106px}.ny{padding-bottom:124.77396021699819%}.nz{max-width:1996px}.oa{padding-bottom:57.815631262525045%}.ob{padding-bottom:58.69947275922671%}.oc{padding-bottom:53.097345132743364%}.od{padding-bottom:58.96309314586995%}.oe{max-width:1116px}.of{padding-bottom:122.04301075268816%}.og{max-width:1008px}.oh{max-width:2016px}.oi{padding-bottom:60.61507936507937%}.oj{max-width:2106px}.ok{padding-bottom:66.38176638176638%}.ol{padding-bottom:33.020833333333336%}.om{padding-bottom:32.86458333333333%}.on{padding-bottom:33.072916666666664%}.oo{will-change:opacity}.op{position:fixed}.oq{width:188px}.or{transform:translateX(406px)}.os{top:calc(65px + 54px + 14px)}.ov{top:calc(65px + 54px + 40px)}.ox{width:131px}.oy{padding-bottom:28px}.oz{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.pa{font-size:18px}.pb{padding-bottom:20px}.pc{padding-top:2px}.pd{max-height:120px}.pe{-webkit-line-clamp:6}.pf{left:6px}.pg{color:rgba(255, 255, 255, 1)}.ph{fill:rgba(255, 255, 255, 1)}.pi{background:rgba(102, 138, 170, 1)}.pj:hover{background:rgba(90, 118, 144, 1)}.pk:disabled{opacity:0.3}.pl:disabled:hover{background:rgba(102, 138, 170, 1)}.pm:disabled:hover{border-color:rgba(102, 138, 170, 1)}.pn{padding-top:28px}.po{margin-bottom:19px}.pp{margin-left:-3px}.pq{margin-right:5px}.pr{outline:0}.ps{border:0}.pt{user-select:none}.pu{cursor:pointer}.pv> svg{pointer-events:none}.pw:active{border-style:none}.px{-webkit-user-select:none}.py:focus{fill:rgba(90, 118, 144, 1)}.pz{margin-top:5px}.qa button{text-align:left}.qb{margin-top:40px}.qc{flex-wrap:wrap}.qd{margin-top:25px}.qe{margin-bottom:8px}.qf{border-radius:3px}.qg{padding:5px 10px}.qh{line-height:22px}.qi{margin-top:15px}.qj{border:1px solid rgba(0, 0, 0, 0.1)}.qk{height:60px}.ql{width:60px}.qy:active{border-style:solid}.qz{z-index:2}.rb{padding-right:6px}.rc{padding-right:8px}.rd{padding-top:32px}.re{border-top:1px solid rgba(0, 0, 0, 0.1)}.rf{margin-bottom:25px}.rg{margin-bottom:32px}.rh{min-height:80px}.rm{width:80px}.rn{padding-left:102px}.rp{letter-spacing:0.05em}.rq{margin-bottom:6px}.rr{line-height:36px}.rs{max-width:555px}.rt{max-width:450px}.ru{line-height:24px}.rw{max-width:550px}.rx{padding-top:25px}.ry{opacity:1}.rz{border:1px solid rgba(102, 138, 170, 1)}.sa{margin-top:64px}.sb{background-color:rgba(0, 0, 0, 0.02)}.sc{padding:60px 0}.sd{background-color:rgba(0, 0, 0, 0.9)}.su{padding-bottom:48px}.sv{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.sw{margin:0 -12px}.sx{margin:0 12px}.sy{flex:1 1 0}.sz{padding-bottom:12px}.ta:hover{color:rgba(255, 255, 255, 0.99)}.tb:hover{fill:rgba(255, 255, 255, 0.99)}.tc:disabled{color:rgba(255, 255, 255, 0.7)}.td:disabled{fill:rgba(255, 255, 255, 0.7)}.te{color:rgba(255, 255, 255, 0.98)}.tf{fill:rgba(255, 255, 255, 0.98)}.tg{text-align:inherit}.th{font-size:21.6px}.ti{letter-spacing:-0.32px}.tj{color:rgba(255, 255, 255, 0.7)}.tk{fill:rgba(255, 255, 255, 0.7)}.tl{text-decoration:underline}.tm{padding-bottom:8px}.tn{padding-top:8px}.to{width:200px}.tq{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.af{margin:0 64px}.gi{font-size:40px}.gj{margin-top:0.78em}.gp{line-height:48px}.gz{font-size:24px}.ha{margin-top:0.79em}.hg{line-height:32px}.it{margin-left:30px}.jg{font-size:21px}.jh{margin-top:2em}.jr{max-width:1192px}.jw{margin-top:56px}.kv{margin-top:1.75em}.lh{font-size:34px}.li{margin-top:1.25em}.lo{margin-top:0.86em}.mf{margin-top:1.05em}.mk{margin-top:1.95em}.ne{font-size:26px}.nf{margin-top:1.72em}.nu{margin-top:1.91em}.sr{padding-left:64px}.ss{padding-right:64px}.st{max-width:1320px}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.is{margin-left:30px}.lr{margin-left:auto}.ls{text-align:center}.so{padding-left:64px}.sp{padding-right:64px}.sq{max-width:1080px}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.ch{display:flex}.ir{margin-left:30px}.sl{padding-left:48px}.sm{padding-right:48px}.sn{max-width:904px}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ak{height:56px}.al{display:flex}.ce{margin-left:10px}.cf{margin-right:10px}.dk{display:block}.ep{margin-bottom:0px}.eq{height:110px}.hk{margin-top:32px}.hl{flex-direction:column-reverse}.ip{margin-bottom:30px}.iq{margin-left:0px}.ri{margin-bottom:24px}.rj{align-items:center}.rk{width:102px}.rl{position:relative}.ro{padding-left:0}.rv{margin-top:24px}.se{padding:32px 0}.si{padding-left:24px}.sj{padding-right:24px}.sk{max-width:728px}.tp{width:140px}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.z{margin:0 24px}.ga{font-size:30px}.gb{margin-top:0.72em}.gl{line-height:40px}.gr{font-size:18px}.gs{margin-top:0.79em}.hc{line-height:24px}.hj{margin-top:32px}.hx{margin-bottom:0px}.in{margin-bottom:30px}.io{margin-left:0px}.ja{margin-top:1.56em}.jk{margin:0}.jl{max-width:100%}.js{margin-top:40px}.kr{margin-top:1.08em}.lb{margin-top:0.93em}.lk{margin-top:0.67em}.mb{margin-top:1.34em}.mg{margin-top:1.2em}.mw{font-size:24px}.mx{margin-top:1.23em}.nq{margin-top:1.41em}.sf{padding-left:24px}.sg{padding-right:24px}.sh{max-width:552px}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ae{margin:0 64px}.gg{font-size:40px}.gh{margin-top:0.78em}.go{line-height:48px}.gx{font-size:24px}.gy{margin-top:0.79em}.hf{line-height:32px}.je{font-size:21px}.jf{margin-top:2em}.jq{max-width:1192px}.jv{margin-top:56px}.ku{margin-top:1.75em}.lf{font-size:34px}.lg{margin-top:1.25em}.ln{margin-top:0.86em}.me{margin-top:1.05em}.mj{margin-top:1.95em}.nc{font-size:26px}.nd{margin-top:1.72em}.nt{margin-top:1.91em}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ac{margin:0 48px}.ge{font-size:40px}.gf{margin-top:0.78em}.gn{line-height:48px}.gv{font-size:24px}.gw{margin-top:0.79em}.he{line-height:32px}.jc{font-size:21px}.jd{margin-top:2em}.jo{margin:0}.jp{max-width:100%}.ju{margin-top:56px}.kt{margin-top:1.75em}.ld{font-size:34px}.le{margin-top:1.25em}.lm{margin-top:0.86em}.md{margin-top:1.05em}.mi{margin-top:1.95em}.na{font-size:26px}.nb{margin-top:1.72em}.ns{margin-top:1.91em}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ab{margin:0 24px}.gc{font-size:30px}.gd{margin-top:0.72em}.gm{line-height:40px}.gt{font-size:18px}.gu{margin-top:0.79em}.hd{line-height:24px}.jb{margin-top:1.56em}.jm{margin:0}.jn{max-width:100%}.jt{margin-top:40px}.ks{margin-top:1.08em}.lc{margin-top:0.93em}.ll{margin-top:0.67em}.mc{margin-top:1.34em}.mh{margin-top:1.2em}.my{font-size:24px}.mz{margin-top:1.23em}.nr{margin-top:1.41em}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="print">.y{display:none}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ot{transition:opacity 200ms}.qm{transition:border-color 150ms ease}.qn::before{background:
      radial-gradient(circle, rgba(90, 118, 144, 1) 60%, transparent 70%)
    }.qo::before{border-radius:50%}.qp::before{content:""}.qq::before{display:block}.qr::before{z-index:0}.qs::before{left:0}.qt::before{height:100%}.qu::before{position:absolute}.qv::before{top:0}.qw::before{width:100%}.qx:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.ra{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 1230px)">.ou{display:none}</style><style type="text/css" data-fela-rehydration="535" data-fela-type="RULE" media="all and (max-width: 1198px)">.ow{display:none}</style><script charset="utf-8" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/vendorstracing.js"></script><script charset="utf-8" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/tracing.js"></script><link rel="icon" href="https://miro.medium.com/fit/c/128/128/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*TWabaG5N1WaKuALbr8lGHw.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785","dateCreated":"2019-02-04T17:00:06.153Z","datePublished":"2019-02-04T17:00:06.153Z","dateModified":"2019-11-12T02:16:28.406Z","headline":"Audio AI: isolating vocals from stereo music using Convolutional Neural Networks","name":"Audio AI: isolating vocals from stereo music using Convolutional Neural Networks","description":"What if we could go back to 1965, knock on Abbey Road Studios’ front door holding an ‘All Access’ badge, and have the privilege of listening to those signature Lennon-McCartney harmonies A-Capella…","identifier":"210532383785","keywords":["Lite:true","Tag:Machine Learning","Tag:Signal Processing","Tag:Audio","Tag:Music","Tag:Towards Data Science","Topic:Artificial Intelligence","Topic:Machine Learning","Topic:Data Science","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_UGC","LayerCake:3"],"author":{"@type":"Person","name":"Ale Koretzky","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@ale.koretzky"},"creator":["Ale Koretzky"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F165\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785","isAccessibleForFree":"False","hasPart":{"@type":"WebPageElement","isAccessibleForFree":"False","cssSelector":".meteredContent"}}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><style type="text/css">.vimvixen-console-frame {
  margin: 0;
  padding: 0;
  bottom: 0;
  left: 0;
  width: 100%;
  height: 100%;
  position: fixed;
  z-index: 2147483647;
  border: none;
  background-color: unset;
  pointer-events: none; }
</style><style type="text/css">.vimvixen-hint {
  background-color: yellow;
  border: 1px solid gold;
  font-weight: bold;
  position: absolute;
  text-transform: uppercase;
  z-index: 2147483647;
  font-size: 12px;
  color: black;
}
</style></head><body data-gr-c-s-loaded="true" cz-shortcut-listen="true"><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y"><div><div class="r c"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="aj n o ak al"><div class="n o am w"><a href="https://medium.com/?source=post_page-----210532383785----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div><div class="r an w"><span class="ao b ap aq ar as r at au"><div class="n o av"><div class="n f"><div class="bd" aria-hidden="true"><div class="n"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25" class="cc cd r ce cf"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="be bf bg bh aq bi bj bk bl" placeholder="Search Towards Data Science"></div></div></div><div class="cg ch"><a href="https://towardsdatascience.com/search?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="cc ci r ce cf"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a href="https://medium.com/me/list/queue?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ci r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="ci n cf"><div class="bd" aria-hidden="true"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb r"><svg width="25" height="25" viewBox="-293 409 25 25" class="cj r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="fu" id="li-post-page-navbar-upsell-button"><div class="ci r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="cl cm q cn co cp cq cr bv ca cs ct cu cv cw cx cy ao b ap aq ar as cz da db bd dc by" rel="noopener">Resume membership</a></div></div></div><div class="n" aria-hidden="true"><div class="dd de n o"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><img alt="Mrityunjay Bhardwaj" class="r df dg dh" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/0OXpS9g7Ercq41VTs.jpeg" width="32" height="32"></button></div></div></div></span></div></div></div></div></div><div class="di r dj dk"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="dl bb n o"><div class="dm r an"><a href="https://towardsdatascience.com/?source=post_page-----210532383785----------------------" rel="noopener"><div class="dn do r"><img alt="Towards Data Science" class="" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1mG6i4Bh_LgixUYXJgQpYsg2x.png" width="100" height="36"></div></a></div><div class="dp r dq"><ul class="dr bu ds bc dt n du g dv dw dx"><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Data Science</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Machine Learning</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/programming/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Programming</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Visualization</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">AI</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/our-picks/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Picks</a></span></li><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/more/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">More</a></span></li><span class="dy ec ed"></span><li class="n o dy dz ea eb"><span class="ao ee ef aq eg eh ei"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ej ek by bz el em" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="en eo r ep eq"></div><article class="meteredContent"><section class="er es et eu ai ev db n ew"></section><span class="r"></span><div><div class="s u ex ey ez fa"></div><div class="et eu ev bj"><div class="r h g f e"><aside class="xh s t" style="width: 568px;"><div class="xk xl s xm bc ai"><h4 class="ao ee ef aq at"><span class="bd xl bc bb ba">Top highlight</span></h4></div></aside></div></div><section class="fb fc fd fe ff"><div class="fg ai"><figure class="fh fg ai paragraph-image"><div class="fi fj bj fk ai"><div class="fp r bj fq"><div class="fr r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1TWabaG5N1WaKuALbr8lGHw_002.png" role="presentation" width="3268" height="1050"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1TWabaG5N1WaKuALbr8lGHw.png" width="3268" height="1050"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/6536/1*TWabaG5N1WaKuALbr8lGHw.png" width="3268" height="1050" role="presentation"/></noscript></div></div></div></figure></div><div class="n p"><div class="z ab ac ae af fw ah ai"><div><div id="2555" class="fx fy cm ap fz b ga gb gc gd ge gf gg gh gi gj gk"><h1 class="fz b ga gl gc gm ge gn gg go gi gp cm">Audio AI: isolating vocals from stereo music using Convolutional Neural Networks</h1></div></div><div id="c145" class="gq fy at ap ao ee gr gs gt gu gv gw gx gy gz ha hb"><h2 class="ao ee gr hc gt hd gv he gx hf gz hg at">hacking music towards the democratization of derivative content</h2></div><div class="hh"><div class="n hi hj hk hl"><div class="o n"><div><a href="https://towardsdatascience.com/@ale.koretzky?source=post_page-----210532383785----------------------" rel="noopener"><div class="bj hm hn"><div class="ho n hp o p s hq hr hs ht hu fa"><svg width="57" height="57" viewBox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="Ale Koretzky" class="r df hn hm" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1RsCKqWhLuOLSzBRiBYEVow.jpeg" width="48" height="48"></div></a></div><div class="hv ai r"><div class="n"><div style="flex:1"><span class="ao b ap aq ar as r cm q"><div class="hw n o hx"><span class="ao ee bh aq bb hy ba hz ia ib cm"><a href="https://towardsdatascience.com/@ale.koretzky?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Ale Koretzky</a></span><div class="id r an h"><button class="ie cn if ig ih ii ij ik bv cy ao b ap il ef as cz da db bd dc by">Follow</button></div></div></span></div></div><span class="ao b ap aq ar as r at au"><span class="ao ee bh aq bb hy ba hz ia ib at"><div><a class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener" href="https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785?source=post_page-----210532383785----------------------">Feb 4, 2019</a> <!-- -->·<!-- --> <!-- -->15<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n im in io ip iq ir is it y"><div class="n o"><div class="iu r"><div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="iw r am"><div class="bd" aria-hidden="true"><div class="bd" aria-hidden="true"><div class="r an"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div></div></div></div><p id="95b7" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">What
 if we could go back to 1965, knock on Abbey Road Studios’ front door 
holding an ‘All Access’ badge, and have the privilege of listening to 
those signature Lennon-McCartney harmonies A-Capella? Our input here is a
 medium quality mp3 of <em class="jj">We Can Work it Out </em>by<em class="jj"> </em>The Beatles. The top track is the input mix and the bottom track, the isolated vocals coming out of our model.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="jx r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media.html" allowfullscreen="" title="vocal isolation from stereo using Convolutional Neural Networks" class="s t u fm ai" scrolling="auto" width="1920" height="787" frameborder="0"></iframe></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="5803" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Formally known as <em class="jj">Audio Source Separation</em>, the problem we are trying to solve here consists in recovering or reconstructing one or more<em class="jj"> source signals</em> that, through some -<em class="jj">linear</em> <em class="jj">or</em> <em class="jj">convolutive-</em>
 process, have been mixed with other signals. The field has many 
practical applications including but not limited to speech denoising and
 enhancement, music remixing, spatial audio, remastering, etc. In the 
context of music production, it is sometimes referred to as <em class="jj">unmixing</em> or <em class="jj">demixing</em>. There’s a good amount of resources on the subject, going from ICA-based -<em class="jj">blind-</em>
 Source Separation, to semi-supervised Non-negative Matrix Factorization
 techniques, to more recent neural network-based approaches. For a nice 
walkthrough on the first two, you can check out <a href="https://ccrma.stanford.edu/~njb/teaching/sstutorial/" class="bm dc jy jz ka kb" target="_blank" rel="noopener nofollow">these tutorial mini-series</a> from CCRMA, which I found very useful back in the day.</p><p id="c191" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc">But before jumping into design stuff.. a liiittle bit of Applied Machine Learning philosophy…</strong></p><p id="2990" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">As someone who’s been working in signal &amp; image processing for a while and prior to the ‘<em class="jj">deep-learning-solves-it-all’ </em>boom, I will introduce the solution as a <strong class="iz kc">Feature Engineering</strong> journey and show you <strong class="iz kc">why, for this particular problem</strong>, an artificial neural network ends up being the best approach. Why? Very often I find people writing things like:</p><p id="8756" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><em class="jj">“with deep learning you don’t have to worry about feature engineering anymore; it does it for you”</em></p><p id="f189" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">or<em class="jj"> </em>worst…</p><p id="7c59" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><em class="jj">“the difference between machine learning and deep learning </em><strong class="iz kc">&lt; </strong>let me stop you right there… Deep Learning is still Machine Learning! <strong class="iz kc">&gt;</strong> <em class="jj">is that in ML you do the feature extraction and in deep learning it happens automatically inside the network”.</em></p><p id="e05c" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">These
 generalizations, probably coming from the fact that DNNs can be pretty 
effective at learning good latent spaces, are just wrong. It frustrates 
me to see recent grads and practitioners being sold on the above 
misconceptions and going for the ‘<em class="jj">deep-learning-solves-it-all’ </em>approach<em class="jj"> </em>as<em class="jj"> </em>something
 you just throw a bunch of raw data at (yes, even after doing some 
pre-processing you can still be a sinner :)), and expect things to just 
work as desired. In the real world, where your data is not as simple, 
clean and pretty as the MNIST dataset and where you have to care about 
things like real-time, memory and so on, these misconceptions can leave 
you stuck in experimentation mode for a very long time…</p></div></div></section><hr class="kd ee ke be kf kg kh ki kj kk kl"><section class="fb fc fd fe ff"><div class="n p"><div class="z ab ac ae af fw ah ai"><blockquote class="km"><div id="b56c" class="kn ko kp ap fz b kq kr ks kt ku kv ji" data-selectable-paragraph=""><p class="fz b kw kx at"><mark class="xi xj pu"><strong class="bs">Feature
 Engineering not only remains a very important discipline when designing
 artificial neural networks; in most cases and just like with any other 
ML technique, it separates </strong></mark><mark class="xi xj pu">production-ready solutions from </mark><mark class="xi xj pu"><strong class="bs">failing or underperforming experiments. A deep understanding of your data and its domain can still get you very far…</strong></mark></p></div></blockquote></div></div></section><hr class="kd ee ke be kf kg kh ki kj kk kl"><section class="fb fc fd fe ff"><div class="n p"><div class="z ab ac ae af fw ah ai"><h1 id="fe11" class="ky kz cm ap ao la ga lb gc lc ld le lf lg lh li lj" data-selectable-paragraph="">From A to Z</h1><p id="9fcc" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Ok,
 now that I’m done preaching, let’s get into what you came for! Just 
like with every other data problem that I’ve worked on in my career, 
I’ll begin by asking the question <strong class="iz kc"><em class="jj">“how does the data look like”?</em></strong>. Let’s take a look at the following fragment of singing voice from an original studio recording.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="jx r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_002.html" allowfullscreen="" title="Studio vocals, Ariana Grande" class="s t u fm ai" scrolling="auto" width="1920" height="787" frameborder="0"></iframe></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee">‘One Last Time’ studio vocals, Ariana Grande</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="c6cf" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Not too interesting right? Well, this is because we are visualizing the waveform or <em class="jj">time-domain signal</em>,
 where all we have access to are the amplitude values of the signal over
 time. We could extract things such as envelopes, RMS values, 
zero-crossing rate, etc, but these <em class="jj">features</em> are <em class="jj">too</em> <em class="jj">primitive </em>and
 not discriminative enough for helping us solve the problem. If we want 
to extract vocal content from a mix we should somehow <strong class="iz kc">expose</strong> <strong class="iz kc">the structure of human speech, </strong>to begin with<strong class="iz kc">. </strong>Luckily, the <a href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" class="bm dc jy jz ka kb" target="_blank" rel="noopener nofollow">Short-Time Fourier Transform (STFT)</a> comes to the rescue.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="lt r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_003.html" allowfullscreen="" title="Studio vocals, Ariana Grande - STFT" class="s t u fm ai" scrolling="auto" width="1920" height="987" frameborder="0"></iframe></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee">STFT magnitude spectrum - window size = 2048, overlap = 75%, log-frequency [Sonic Visualizer]</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="565c" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Although I love Speech Processing and I would definitely enjoy going through <em class="jj">source-filter modeling, cepstrum, quefrencies, LPC, MFCC</em>
 and so on, I’ll skip all that stuff and focus on the core elements 
related to our problem, so that the article is digestible by as many 
people as possible and not exclusively by the Audio Signal Processing / 
Speech community.</p><p id="af89" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">So, what does the structure of human speech tell us?</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu lu"><div class="fp r bj fq"><div class="lx r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1zH6VF4UG4xw6s6XWjqz2pg.png" role="presentation" width="2862" height="1452"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1zH6VF4UG4xw6s6XWjqz2pg_002.png" width="2862" height="1452"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/5724/1*zH6VF4UG4xw6s6XWjqz2pg.png" width="2862" height="1452" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="03d2" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Well, there are 3 main elements that we can identify here:</p><ul class=""><li id="ae69" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji ly lz ma" data-selectable-paragraph="">A <strong class="iz kc">fundamental frequency</strong> (f0), determined by the frequency of vibration of our vocal cords. In this case, Ariana is singing in the 300–500 Hz range.</li><li id="a104" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">A number of <strong class="iz kc">harmonics</strong> above f0, following a similar <em class="jj">shape</em> or pattern. These harmonics happen at integer multiples of f0.</li><li id="71c5" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph=""><strong class="iz kc">unvoiced</strong> speech, which includes consonants like<em class="jj"> ‘t’, ‘p’, ‘k’</em>,
 ‘s’ (which are not produced by the vibration of our vocal cords), 
breaths, etc. They manifest as short bursts in the high-frequency 
region.</li></ul><h1 id="f8fa" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph=""><strong class="bs">A first shot using a rule-based approach</strong></h1><p id="c182" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Let’s
 forget for a second about this thing called Machine Learning. Based on 
our knowledge about the data, can we come up with a method to extract 
our vocals? Let me give it a try…</p><p id="d7ed" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc"><em class="jj">Naive</em> vocal isolation V1.0:</strong></p><ol class=""><li id="d663" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji ml lz ma" data-selectable-paragraph="">Identify
 vocal sections. There’s a lot going on within a mix. We want to focus 
on the sections that actually contain vocal content and ignore the rest.</li><li id="9d5d" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ml lz ma" data-selectable-paragraph="">Distinguish
 between voiced and unvoiced sections. As we saw, voiced speech looks 
very different from unvoiced speech, therefore they probably need 
different treatment.</li><li id="6994" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ml lz ma" data-selectable-paragraph="">Estimate the frequency of the fundamental over time.</li><li id="3137" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ml lz ma" data-selectable-paragraph="">Based on the output of 3, apply some sort of mask to capture the harmonic content.</li><li id="0cc7" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ml lz ma" data-selectable-paragraph="">Do something else about the unvoiced sections…</li></ol></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu ag"><div class="fp r bj fq"><div class="mm r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1V30mUf8OHZNaWrHxluUbEg.jpeg" role="presentation" width="1050" height="603"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1V30mUf8OHZNaWrHxluUbEg.gif" width="1050" height="603"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2100/1*V30mUf8OHZNaWrHxluUbEg.gif" width="1050" height="603" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="97cf" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">If we do a decent job, the output of this process should be a <em class="jj">soft</em> or <em class="jj">binary</em> <em class="jj">mask</em>
 that, when applied (element-wise multiplication) to the magnitude STFT 
of the mix, gives us an approximate reconstruction of the magnitude STFT
 of the vocals. From there, we then combine this vocal STFT estimate 
with the phase information of the original mix, compute an inverse STFT,
 and obtain the time-domain signal of the reconstructed vocals.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu mn"><div class="fp r bj fq"><div class="mo r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1bKZbYE0YQ2u4SpgJlHvJnA.png" role="presentation" width="1805" height="1445"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1bKZbYE0YQ2u4SpgJlHvJnA_002.png" width="1805" height="1445"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/3610/1*bKZbYE0YQ2u4SpgJlHvJnA.png" width="1805" height="1445" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="d495" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Doing
 this from scratch is already a lot of work. But for the sake of the 
demonstration, we’re going to use an implementation of the <a href="https://code.soundsoftware.ac.uk/projects/pyin" class="bm dc jy jz ka kb" target="_blank" rel="noopener nofollow">pYIN algorithm</a>.
 Even though it’s meant for solving step 3, with the right constraints 
it takes care of 1 and 2 pretty decently, while tracking the vocal 
fundamental even in the presence of music. The example below contains 
the output from this approach, without addressing the unvoiced sections.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="mp r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_004.html" allowfullscreen="" title="ariana harmonic mask" class="s t u fm ai" scrolling="auto" width="1920" height="849" frameborder="0"></iframe></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="cba7" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Well…?
 It sort of did the trick but the quality of the recovered vocal is not 
there yet. Maybe with additional time, energy and budget we can improve 
this method and get it to a better place.</p><p id="8701" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Now let me ask you…</p><p id="c371" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">What happens when you have <strong class="iz kc">multiple vocals</strong>, which is definitely the case in at least 50% of today’s professionally produced tracks?</p><p id="c427" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">What happens when the vocals have been processed with <strong class="iz kc">reverberation</strong>, <strong class="iz kc">delays</strong> and other effects? Let’s take a look at the last chorus of Ariana Grande’s <em class="jj">One Last Time.</em></p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="mq r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_005.html" allowfullscreen="" title="Ariana Grande 'One Last Time' - Multiple vocals (last chorus)" class="s t u fm ai" scrolling="auto" width="1920" height="995" frameborder="0"></iframe></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="81fc" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Are you feeling the pain already…? I am.</p><p id="d8f5" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Very
 soon, ad-hoc methods like the one described above become a house of 
cards. The problem is just too complex. There are too many rules, too 
many exceptions to the rules and too many varying conditions (effects 
and different mixing settings). The multi-step approach also implies 
that errors in one step propagate issues to the step that comes after. 
Improving each step would be very costly, it would require a large 
number of iterations to get right and last but not least, we would 
probably end up with a computationally expensive pipeline, something 
that by itself can be a deal-breaker.</p><p id="28f2" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc">These are the kind of scenarios where we need to start thinking of a more <em class="jj">end-to-end</em>
 approach and let ML figure out -PART- of the underlying processes and 
operations required to solve the problem. However, we are not throwing 
the towel when it comes to feature engineering and you’ll see why.</strong></p><h1 id="854b" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph="">The hypothesis: we can use a CNN as a transfer function that maps mixes into vocals</h1><p id="2261" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Inspired by the achievements with CNNs on natural images, why not apply the same reasoning here?</p><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu mr"><div class="fp r bj fq"><div class="ms r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1L86EVDCdz3HQ5NpxfPWLTg_002.png" role="presentation" width="1070" height="1070"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1L86EVDCdz3HQ5NpxfPWLTg.png" width="1070" height="1070"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2140/1*L86EVDCdz3HQ5NpxfPWLTg.png" width="1070" height="1070" role="presentation"/></noscript></div></div></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee" data-selectable-paragraph="">CNNs have been successful at tasks such as image colorization, deblurring and super-resolution.</figcaption></figure><p id="fa1e" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">At the end of the day, we know we can represent an audio signal ‘as an image<em class="jj">’ </em>using the Short-Time Fourier Transform right? Even though these <em class="jj">audio</em> <em class="jj">images</em>
 don’t follow the statistical distribution of natural images, they still
 expose spatial patterns (in the time vs frequency space) that we should
 be able to learn from.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu mt"><div class="fp r bj fq"><div class="mu r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1X0nGpE5CkQgYJnINcALD_w.png" role="presentation" width="1706" height="992"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1X0nGpE5CkQgYJnINcALD_w_002.png" width="1706" height="992"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/3412/1*X0nGpE5CkQgYJnINcALD_w.png" width="1706" height="992" role="presentation"/></noscript></div></div></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee" data-selectable-paragraph="">Mix:
 you can see the kick drum and baseline at the bottom, and some of the 
synths in the middle getting mixed with the vocals. On the right, the 
corresponding vocals-only</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="78bc" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">At
 the time, validating this experiment was a costly endeavor, because 
obtaining or generating the training data required was already a big 
investment. One of the practices I always try to implement in applied 
research is to first <strong class="iz kc">identify a simpler problem that validates the same principles</strong> <strong class="iz kc">as the original one, </strong>but
 that does not require as much work. This allows you to keep your 
hypotheses smaller, iterate faster and pivot with minimum impact when 
things don’t work as expected.</p><p id="1105" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">An implied condition for the original solution to work is that a <strong class="iz kc">CNN must be capable of understanding the structure of human speech</strong>. A simpler problem can then be: <em class="jj">given a mix fragment, let’s see if a CNN can classify these fragments as containing vocal content or not</em>. We are looking at a music-robust <a href="https://en.wikipedia.org/wiki/Voice_activity_detection" class="bm dc jy jz ka kb" target="_blank" rel="noopener nofollow">Vocal Activity Detector (VAD)</a>, implemented as a binary classifier.</p><h2 id="2719" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph="">Designing our feature space</h2><p id="0b3d" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">We
 know that audio signals such as music and human speech embed temporal 
dependencies. In simpler terms, nothing happens in isolation at a given 
time-frame. If I want to know whether a given section of audio contains 
human speech or not, I should probably look at the neighbor regions as 
well. That <em class="jj">temporal context</em> can give me good 
information about what’s going on in the region of interest. At the same
 time, we want to perform our classification in very small time 
increments, so that we can capture the human voice with the highest 
temporal resolution possible.</p><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu nh"><div class="fp r bj fq"><div class="ni r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/14ZoIG-CYTxz6nTtvVlZv2g.jpeg" role="presentation" width="1296" height="684"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/14ZoIG-CYTxz6nTtvVlZv2g.gif" width="1296" height="684"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2592/1*4ZoIG-CYTxz6nTtvVlZv2g.gif" width="1296" height="684" role="presentation"/></noscript></div></div></div></div></figure><p id="1572" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Let’s do some numbers…</p><ul class=""><li id="a93d" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji ly lz ma" data-selectable-paragraph="">Sampling rate (fs): 22050 Hz (we are downsampling from 44100 to 22050)</li><li id="ea4b" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">STFT design: window size = 1024, hop size = 256, <a href="https://en.wikipedia.org/wiki/Mel_scale" class="bm dc jy jz ka kb" target="_blank" rel="noopener nofollow">Mel scale </a>interpolation for perceptual weighting. Since our input data is <em class="jj">real, </em>we
 can work with one half of the STFT (the why is out of the scope of this
 post…) while keeping the DC component (not a requirement), giving us 
513 frequency bins.</li><li id="e9cc" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">Target classification resolution: a single STFT frame (~11.6 milliseconds = 256 / 22050)</li><li id="bbd5" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">Target temporal context: ~300 milliseconds = 25 STFT frames.</li><li id="a9c9" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">Target number of training examples: 0.5M</li><li id="9023" class="ix iy cm ap iz b gr mb gt mc jc md je me jg mf ji ly lz ma" data-selectable-paragraph="">Assuming
 we are using a sliding window with a stride of 1 STFT time-frame to 
generate our training data, we need around 1.6 hours of labeled audio to
 generate our 0.5M data samples. [if you’d like to know more details 
about generating the actual dataset, please feel free to ask in the 
comments]</li></ul><p id="0306" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">With the above requirements, the input/output data to our binary classifier looks like this:</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu ag"><div class="fp r bj fq"><div class="nj r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1Excr3WeAH5ZUot-RnL0WOA_002.png" role="presentation" width="1191" height="674"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1Excr3WeAH5ZUot-RnL0WOA.png" width="1191" height="674"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2382/1*Excr3WeAH5ZUot-RnL0WOA.png" width="1191" height="674" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><h2 id="8d0a" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph=""><strong class="bs">The model</strong></h2><p id="2705" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Using Keras, we can build a small CNN model to validate our hypothesis.</p><pre class="js jt ju jv jw nk nl dt"><span id="f912" class="mv kz cm ap nm b bh nn no r np" data-selectable-paragraph="">import keras<br>from keras.models import Sequential<br>from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D<br>from keras.optimizers import SGD<br>from keras.layers.advanced_activations import LeakyReLU</span><span id="770d" class="mv kz cm ap nm b bh nq nr ns nt nu no r np" data-selectable-paragraph="">model = Sequential()<br>model.add(Conv2D(16, (3,3), padding='same', input_shape=(513, 25, 1)))<br>model.add(LeakyReLU())<br>model.add(Conv2D(16, (3,3), padding='same'))<br>model.add(LeakyReLU())<br>model.add(MaxPooling2D(pool_size=(3,3)))<br>model.add(Dropout(0.25))</span><span id="ecec" class="mv kz cm ap nm b bh nq nr ns nt nu no r np" data-selectable-paragraph="">model.add(Conv2D(16, (3,3), padding='same'))<br>model.add(LeakyReLU())<br>model.add(Conv2D(16, (3,3), padding='same'))<br>model.add(LeakyReLU())<br>model.add(MaxPooling2D(pool_size=(3,3)))<br>model.add(Dropout(0.25))</span><span id="7016" class="mv kz cm ap nm b bh nq nr ns nt nu no r np" data-selectable-paragraph="">model.add(Flatten())<br>model.add(Dense(64))<br>model.add(LeakyReLU())<br>model.add(Dropout(0.5))<br>model.add(Dense(1, activation='sigmoid'))</span><span id="5cbc" class="mv kz cm ap nm b bh nq nr ns nt nu no r np" data-selectable-paragraph="">sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)<br>model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=['accuracy'])</span></pre><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="et eu nv"><div class="fp r bj fq"><div class="nw r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1mPtHH7QbrxeNPxInt0wM6g_002.png" role="presentation" width="535" height="1907"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1mPtHH7QbrxeNPxInt0wM6g.png" width="535" height="1907"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/1070/1*mPtHH7QbrxeNPxInt0wM6g.png" width="535" height="1907" role="presentation"/></noscript></div></div></div></figure><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu nx"><div class="fp r bj fq"><div class="ny r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1HBtu3jtQ8k9vkaYFPDTN3A_002.png" role="presentation" width="1106" height="1380"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1HBtu3jtQ8k9vkaYFPDTN3A.png" width="1106" height="1380"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2212/1*HBtu3jtQ8k9vkaYFPDTN3A.png" width="1106" height="1380" role="presentation"/></noscript></div></div></div></div></figure><p id="5096" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">With an 80/20 train-test split and after ~50 epochs we reach ~<strong class="iz kc">97% test accuracy</strong>,
 which means there’s enough evidence that our CNN model can discriminate
 between music sections containing vocal content and music sections 
without vocal content. By inspecting some of the feature maps coming out
 of our 4th convolutional layer, it looks like our network has optimized
 its kernels to perform 2 tasks: filtering out music and filtering out 
vocals…</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu nz"><div class="fp r bj fq"><div class="oa r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1C6GmkQuWG-AZoZfR9PNUhg_002.png" role="presentation" width="1996" height="1154"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1C6GmkQuWG-AZoZfR9PNUhg.png" width="1996" height="1154"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/3992/1*C6GmkQuWG-AZoZfR9PNUhg.png" width="1996" height="1154" role="presentation"/></noscript></div></div></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee" data-selectable-paragraph="">Sample
 feature maps at the output of the 4th conv. layer. Apparently, the 
output on the left is the result of a combination of kernel operations 
that try to preserve vocal content while ignoring music. The high values
 resemble the harmonic structure of human speech. The feature map on the
 right seems to be the result of the opposite task.</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><h1 id="bf5a" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph=""><strong class="bs">From VAD to Source Separation</strong></h1><p id="aabb" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Now
 that we’ve validated this simpler classification problem, how do we go 
from detecting vocal activity in music all the way to isolating vocals 
from music? Well, rescuing some ideas from our <em class="jj">naive</em>
 method described at the beginning, we still want to somehow end up with
 an estimate of the vocal’s magnitude spectrogram. This now becomes a 
regression problem. What we want to do is, given a time-frame from the 
STFT of the mix (with enough temporal context), estimate the 
corresponding vocal time-frame’s magnitude spectrum.</p><p id="2274" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc">What about the training set? (you might be asking yourself at this point)</strong></p><p id="d68b" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">oh Lord… that was something. I’m gonna address this at the end of the article so that we don’t switch contexts yet!</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu ag"><div class="fp r bj fq"><div class="ob r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1N_Vy2dGenOEbjRWvgHJ-EA.png" role="presentation" width="1138" height="668"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1N_Vy2dGenOEbjRWvgHJ-EA_002.png" width="1138" height="668"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2276/1*N_Vy2dGenOEbjRWvgHJ-EA.png" width="1138" height="668" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="ea6e" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">If
 our model learns well, during inference, all we need to do is implement
 a simple sliding window over the STFT of the mix. After each 
prediction, we move our window to the right by 1 time-frame, predict the
 next vocal frame and concatenate it with the previous prediction. In 
regards to the model, we can start by using the same model we used for 
VAD as a baseline and by making some changes (output shape is now 
(513,1), linear activation at the output, MSE as loss function), we can 
begin our training.</p><p id="8551" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc">Don’t claim victory yet…</strong></p><p id="73a3" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Although
 the above input/output representation makes sense, after training our 
vocal separation model several times, with varying parameters and data 
normalizations, the results are not there yet. It seems like <strong class="iz kc">we are asking for too much…</strong></p><p id="5134" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">We went from a binary classifier to trying to do <em class="jj">regression</em>
 on a 513-dimensional vector. Although the network learns the task to a 
degree, after reconstructing the vocal’s time domain signal, there are 
obvious artifacts and interferences from other sources. Even after 
adding more layers and increasing the number of model parameters, the 
results don’t change much.</p><p id="6d57" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">So then the question became:<strong class="iz kc"> can we trick the network into <em class="jj">thinking</em> it is solving a simpler problem and still achieve the desired results?</strong></p><p id="ff6d" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">What if instead of trying to estimate the vocal’s magnitude STFT, we trained the network to learn a <em class="jj">binary mask</em> that, when applied to the STFT of the mix, gives us a simplified but <strong class="iz kc">perceptually-acceptable-upon-reconstruction</strong> estimate of the vocal’s magnitude spectrogram?</p><p id="7030" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">By
 experimenting with different heuristics, we came up with a relatively 
simple (and definitely unorthodox from a Signal Processing perspective) 
way to extract singing voice from mixes using binary masks. Without 
going too much into the details, we are going to think of the output as a
 binary <em class="jj">image</em> where, a value of ‘1’ indicates <strong class="iz kc">predominant presence of vocal content </strong>at
 a given frequency and timeframe location, and a value of ‘0’ indicates 
predominant presence of music at the given location. Visually, it looks 
pretty unattractive, but upon reconstructing the time domain signal, the
 results are surprisingly good.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu ag"><div class="fp r bj fq"><div class="oc r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1wo5TZTh9cGLCJM96nSWpdA_002.png" role="presentation" width="1130" height="600"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1wo5TZTh9cGLCJM96nSWpdA.png" width="1130" height="600"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2260/1*wo5TZTh9cGLCJM96nSWpdA.png" width="1130" height="600" role="presentation"/></noscript></div></div></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee" data-selectable-paragraph="">Binary
 masking (or TF masking) techniques have been around for decades and 
have been proven very effective in specific audio source separation and 
classification problems. However, most available techniques have been 
optimized for speech enhancement / recognition applications and fall 
short when it comes to real-world data such as professionally-produced 
&amp; mastered music. Our binarization method was specifically designed 
for this scenario.</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="8a5a" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Our problem now becomes some sort of regression-classification hybrid. We are asking the model to “classify pixels<em class="jj">” </em>at
 the output as vocal or non-vocal, although conceptually (and also in 
terms of the loss function used -MSE- ), the task is still a regression 
one.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu ag"><div class="fp r bj fq"><div class="od r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/15IBjtoObD4Ark2KTqF89nw_002.png" role="presentation" width="1138" height="671"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/15IBjtoObD4Ark2KTqF89nw.png" width="1138" height="671"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2276/1*5IBjtoObD4Ark2KTqF89nw.png" width="1138" height="671" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="94cb" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Although
 the distinction might not seem relevant to some, it actually makes a 
huge difference in the model’s ability to learn the assigned task, the 
second one being way more simple and constrained. At the same time, it 
allows us to keep our model relatively small in terms of number of 
parameters considering the complexity of the task, something highly 
desired for real-time operation, which was a design requirement in this 
case. After some minor tweaks, the final model looks like this.</p><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="et eu nv"><div class="fp r bj fq"><div class="nw r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1ny8Wj6YKPYQ7PVBI6UxOrg_002.png" role="presentation" width="535" height="1907"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1ny8Wj6YKPYQ7PVBI6UxOrg.png" width="535" height="1907"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/1070/1*ny8Wj6YKPYQ7PVBI6UxOrg.png" width="535" height="1907" role="presentation"/></noscript></div></div></div></figure><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu oe"><div class="fp r bj fq"><div class="of r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1fBErlCqo-nvKUwB7e2g5tg.png" role="presentation" width="1116" height="1362"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1fBErlCqo-nvKUwB7e2g5tg_002.png" width="1116" height="1362"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2232/1*fBErlCqo-nvKUwB7e2g5tg.png" width="1116" height="1362" role="presentation"/></noscript></div></div></div></div></figure><h2 id="eeef" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph="">How do we reconstruct the time-domain signal?</h2><p id="1bce" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Basically, as described in the <em class="jj">naive</em> <em class="jj">method</em>
 section. In this case, for every inference pass that we do, we are 
predicting a single timeframe of the vocals’ binary mask. Again, by 
implementing a simple sliding window with a stride of one timeframe, we 
keep estimating and concatenating consecutive timeframes, which end up 
making up the whole vocal binary mask.</p><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu og"><div class="fp r bj fq"><div class="ms r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1iD_RQh916DnUqLm0rphjZw.jpeg" role="presentation" width="1008" height="1008"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1iD_RQh916DnUqLm0rphjZw.gif" width="1008" height="1008"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/2016/1*iD_RQh916DnUqLm0rphjZw.gif" width="1008" height="1008" role="presentation"/></noscript></div></div></div></div></figure><h2 id="1688" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph=""><strong class="bs">Creating the training set</strong></h2><p id="e797" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">As
 you know, one of the biggest pain points in supervised Machine Learning
 (leave aside all those toy examples with available datasets out there) 
is having the right data (amount and quality) for the particular problem
 that you’re trying to solve. Based on the input/output representations 
described, in order to train our model, we first needed a significant 
number of mixes and their corresponding, perfectly aligned and 
normalized vocal tracks. There’s more than one way to build this dataset
 and here we used a combination of strategies, ranging from manually 
creating mix &lt;&gt; vocal pairs with some acapellas found online, to 
finding RockBand stems, to web-scraping Youtube. Just to give you an 
idea of what part of this time-consuming and painful process looked 
like, our “dataset project” involved creating a tool to automatically 
build mix &lt;&gt; vocal pairs as illustrated below:</p><figure class="js jt ju jv jw fg et eu paragraph-image"><div class="fi fj bj fk ai"><div class="et eu oh"><div class="fp r bj fq"><div class="oi r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1qQiiiSVHYzOuiOr1DU4QxQ_002.png" role="presentation" width="2016" height="1222"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1qQiiiSVHYzOuiOr1DU4QxQ.png" width="2016" height="1222"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/4032/1*qQiiiSVHYzOuiOr1DU4QxQ.png" width="2016" height="1222" role="presentation"/></noscript></div></div></div></div></figure><p id="596c" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">We
 knew we needed a good amount of data for the network to learn the 
transfer function needed to map mixes into vocals. Our final dataset 
consisted of around 15M examples of ~300-millisecond fragments of mixes 
and their corresponding vocal binary masks.</p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg lv lw paragraph-image"><div class="fi fj bj fk ai"><div class="et eu oj"><div class="fp r bj fq"><div class="ok r"><div class="bi fl s t u fm ai bb fn fo"><img class="s t u fm ai fs ft ck xg" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1OwMFUHdQb_Cq1gN0qfAwcg_002.png" role="presentation" width="2106" height="1398"></div><img class="ry xf s t u fm ai fv" role="presentation" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1OwMFUHdQb_Cq1gN0qfAwcg.png" width="2106" height="1398"><noscript><img class="s t u fm ai" src="https://miro.medium.com/max/4212/1*OwMFUHdQb_Cq1gN0qfAwcg.png" width="2106" height="1398" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><h2 id="819c" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph="">Pipeline architecture</h2><p id="dbb3" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">Building
 a Machine Learning model for a given task is only part of the deal. In 
production environments, we need to think about software architecture, 
pipelines, and optimization strategies, especially when we’re dealing 
with real-time.</p><p id="66ed" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">For
 this particular problem the reconstruction into the time-domain can be 
done all at once after predicting a full vocal binary mask (offline 
mode) or, more interestingly, as part of a multithreaded pipeline where 
we acquire, process, reconstruct and playback in small segments, 
allowing this to be streaming-friendly, and even capable of delivering 
real-time deconstruction on music that’s being recorded on the fly, with
 minimum latency. Given this is a whole topic on its own, I’m going to 
leave it for another article focused on <strong class="iz kc">real-time ML pipelines</strong>…</p><h1 id="fabe" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph=""><strong class="bs">I think I’ve covered enough so why don’t we listen to a couple more examples?</strong></h1><h2 id="087c" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph="">Daft Punk — Get Lucky (Studio)</h2></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="ol r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_006.html" allowfullscreen="" title="'Daft Punk - Get Lucky', vocal isolation from stereo using Convolutional Neural Networks" class="s t u fm ai" scrolling="auto" width="1920" height="634" frameborder="0"></iframe></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee">we can hear some minimal interference from the drums here…</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><h2 id="159d" class="mv kz cm ap ao la mw mx my mz na nb nc nd ne nf ng" data-selectable-paragraph="">Adele — Set Fire to the Rain (live recording!)</h2></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="om r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_007.html" allowfullscreen="" title="'Adele - Set Fire to the Rain', vocal isolation from stereo using Convolutional Neural Networks" class="s t u fm ai" scrolling="auto" width="1920" height="631" frameborder="0"></iframe></div></div><figcaption class="at bh lp lq kg ev et eu lr ls ao ee">Notice
 how at the very beginning our model extracts the crowd’s screaming as 
vocal content :). In this case we have some additional interference from
 other sources. This being a live recording it kinda makes sense for 
this extracted vocal not to be as high quality as the previous ones.</figcaption></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><h1 id="8bf9" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph="">Ok, so there’s ‘one last thing’ …</h1><h1 id="826f" class="ky kz cm ap ao la ga mg gc mh ld mi lf mj lh mk lj" data-selectable-paragraph="">Given this works for vocals why not apply it to other instruments…?</h1><p id="996f" class="ix iy cm ap iz b gr lk gt ll jc lm je ln jg lo ji fb" data-selectable-paragraph="">This
 article is extensive enough already but given you’ve made it this far I
 thought you deserved to see one last demo. With the exact same 
reasoning for extracting vocal content, we can try to split a stereo 
track into STEMs (drums, bassline, vocals, others) by making some 
modifications to our model and of course, by having the appropriate 
training set :). If you are interested in the technical details for this
 extension, just leave me some comments. I will consider writing a ‘part
 2’ for the Stereo-to-Stems deconstruction case when time allows!</p><p id="ad72" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph=""><strong class="iz kc">*** UPDATE 06/2019***</strong><br>Part 2, Stereo-to-Stems deconstruction is<strong class="iz kc"> </strong><a class="bm dc jy jz ka kb" target="_blank" rel="noopener" href="https://towardsdatascience.com/audio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de"><strong class="iz kc">here</strong></a><strong class="iz kc">!</strong></p></div></div><div class="fg"><div class="n p"><div class="jk jl jm jn jo jp ae jq af jr ah ai"><figure class="js jt ju jv jw fg"><div class="fp r bj"><div class="on r"><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/media_008.html" allowfullscreen="" title="'Daft Punk - Get Lucky' STEMs isolation from stereo using Convolutional Neural Networks" class="s t u fm ai" scrolling="auto" width="1920" height="635" frameborder="0"></iframe></div></div></figure></div></div></div><div class="n p"><div class="z ab ac ae af fw ah ai"><p id="ff66" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">Thanks for reading and don’t hesitate in leaving questions. I will keep writing articles on <strong class="iz kc">Audio AI </strong>so
 stay tuned! As a final remark, you can see that the actual CNN model we
 ended up building is not that special. The success of this work has 
been driven by focusing on the <strong class="iz kc">Feature Engineering</strong> aspect and by implementing a lean process for hypotheses validations, something I’ll be writing about in the near future!</p><p id="b3ac" class="ix iy cm ap iz b gr ja gt jb jc jd je jf jg jh ji fb" data-selectable-paragraph="">ps: shoutouts to Naveen Rajashekharappa and Karthiek Reddy Bokka for their contributions to this work!</p></div></div></section></div></article><div class="bi fa oo op ai ov ot ow" data-test-id="post-sidebar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="ox n ew"><div class="fa"><div class="oy oz r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener"><h2 class="ao la pa aq cm">Towards Data Science</h2></a><div class="pb pc r"><h4 class="ao ee bh aq bb pd ba hz pe ib at">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="bd" aria-hidden="true"><button class="cl pg ph pi ih pj ik bv cu pk pl pm cy ao b ap aq ar as cz da db bd dc by">Following<span class="bj pf"><svg width="21" height="21" viewBox="0 0 21 21"><path d="M4 7.33L10.03 14l.5.55.5-.55 5.96-6.6-.98-.9-5.98 6.6h1L4.98 6.45z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="pn po pp n"><div class="n o"><div class="pq r bj"><div class=""><button class="bt pr ps pt pu pv pw px ig py ij"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="pz r"><div class="qa"><h4 class="ao ee bh aq at"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb">3.4K </button></h4></div></div></div></div><div class="po r"></div><div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="bi fa oo op oq ht or os ot ou"></div><div><div class="qb fg n ew p"><div class="n p"><div class="z ab ac ae af fw ah ai"><div class="n qc"></div><div class="n o qc"></div><div class="qd r"><ul class="bt bu"><li class="bd dr iu qe"><a href="https://towardsdatascience.com/tag/machine-learning" class="qf qg dc at r nl qh a b ef">Machine Learning</a></li><li class="bd dr iu qe"><a href="https://towardsdatascience.com/tag/signal-processing" class="qf qg dc at r nl qh a b ef">Signal Processing</a></li><li class="bd dr iu qe"><a href="https://towardsdatascience.com/tag/audio" class="qf qg dc at r nl qh a b ef">Audio</a></li><li class="bd dr iu qe"><a href="https://towardsdatascience.com/tag/music" class="qf qg dc at r nl qh a b ef">Music</a></li><li class="bd dr iu qe"><a href="https://towardsdatascience.com/tag/towards-data-science" class="qf qg dc at r nl qh a b ef">Towards Data Science</a></li></ul></div><div class="qi n hi y"><div class="n o"><div class="ci r bj"><div class=""><div class="c qj df n o qk bj ql qm qn qo qp qq qr qs qt qu qv qw qx ik"><button class="bt pr ps pt pu pv qy px o fv df n p qz u fm s t ai ig py ij ra"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="pz r"><div class="qa"><h4 class="ao ee bh aq cm"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb">3.4K claps</button></h4></div></div></div><div class="n o"><div class="rb r an"><a href="https://medium.com/p/210532383785/share/twitter?source=post_actions_footer---------------------------" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="rb r an"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="rb r an"><a href="https://medium.com/p/210532383785/share/facebook?source=post_actions_footer---------------------------" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="rc r an"><div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="bd" aria-hidden="true"><div class="bd" aria-hidden="true"><div class="r an"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="rd re rf qd r y"><div class="rg rh r bj"><span class="r ri al rj"><div class="r s rk rl"><a href="https://towardsdatascience.com/@ale.koretzky?source=follow_footer--------------------------follow_footer-" rel="noopener"><div class="bj rm dx"><div class="ho n hp o p s hq hr hs ht hu fa"><svg width="91" height="91" viewBox="0 0 91 91"><path fill-rule="evenodd" clip-rule="evenodd" d="M45.5 1.4c-17.14 0-32 9.95-39.25 24.5L5 25.28C12.47 10.28 27.8 0 45.5 0S78.53 10.29 86 25.28l-1.25.62C77.5 11.35 62.65 1.4 45.5 1.4zM6.25 65.1c7.25 14.55 22.1 24.5 39.25 24.5 17.14 0 32-9.95 39.25-24.5l1.25.62C78.53 80.72 63.2 91 45.5 91S12.47 80.71 5 65.72l1.25-.62z"></path></svg></div><img alt="Ale Koretzky" class="r df dx rm" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1RsCKqWhLuOLSzBRiBYEVow_002.jpeg" width="80" height="80"></div></a></div><span class="r"><div class="rn r ro"><p class="ao ee ef aq at eh rp">Written by</p></div><div class="rn rq n ro"><div class="ai n o hi"><h2 class="ao la ke rr cm"><a href="https://towardsdatascience.com/@ale.koretzky?source=follow_footer--------------------------follow_footer-" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener">Ale Koretzky</a></h2><div class="r g"><button class="cl cn if ig ih ii ij ik bv cy ao b ap aq ar as cz da db bd dc by">Follow</button></div></div></div></span></span><div class="rn rs r ro dk"><div class="rt r"><h4 class="ao ee pa ru at">Head of Machine Learning @Splice. Undercover biz-product guy. Advisor. Made in 🇦🇷, living in Venice, CA.</h4></div><div class="cg rv dk"><button class="cl cn if ig ih ii ij ik bv cy ao b ap aq ar as cz da db bd dc by">Follow</button></div></div></div><div class="rd r"></div><div class="rg rh r bj"><span class="r ri al rj"><div class="r s rk rl"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="cy rm dx" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/1hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="rn rq n ro"><div class="ai n o hi"><h2 class="ao la ke rr cm"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="bd" aria-hidden="true"><button class="cl pg ph pi ih pj ik bv cu pk pl pm cy ao b ap aq ar as cz da db bd dc by">Following<span class="bj pf"><svg width="21" height="21" viewBox="0 0 21 21"><path d="M4 7.33L10.03 14l.5.55.5-.55 5.96-6.6-.98-.9-5.98 6.6h1L4.98 6.45z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></span></span><div class="rn rw r ro dk"><div class="rt r"><h4 class="ao ee pa ru at">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="cg rv dk"><div class="bd" aria-hidden="true"><button class="cl pg ph pi ih pj ik bv cu pk pl pm cy ao b ap aq ar as cz da db bd dc by">Following<span class="bj pf"><svg width="21" height="21" viewBox="0 0 21 21"><path d="M4 7.33L10.03 14l.5.55.5-.55 5.96-6.6-.98-.9-5.98 6.6h1L4.98 6.45z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div><div class="rx re r y"><a href="https://medium.com/p/210532383785/responses/show?source=follow_footer--------------------------follow_footer-" class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener"><span class="kp ry pu"><div class="nk rz cy r kg dk"><span class="if">See responses (55)</span></div></span></a></div></div></div><div class="sa r sb y"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="xq r xr"><div class="tm oz rg r"><h2 class="ao la xs xt cm">More From Medium</h2></div><div class="du n hp qc xu xv xw xx xy xz ya yb yc yd ye yf yg yh yi"><div class="yj yk yl jl ym yn yo jn yp yq yr jp ys yt yu yv yw yx yy yz za"><div class="ai fm"><div class="r zb"><div class="zc zd xu xv xw ze zf xx xy xz zg zh ya yb yc zi zj yd ye yf zk zl yg yh yi n qc"><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="zu r uf f"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><div class="ax r zv xr"><a class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb r" rel="noopener" href="https://towardsdatascience.com/top-3-python-functions-you-dont-know-about-probably-978f4be1e6d?source=post_recirc---------0------------------"><div class="zw bj"><div class="fm s ai"><div class="zx r zy zz fm ai aba xl"></div></div></div></a></div></div><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="ax r"><div class="abb cg h abc"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/top-3-python-functions-you-dont-know-about-probably-978f4be1e6d?source=post_recirc---------0------------------"><h3 class="cm q fz vl ap kq abd abe">Top 3 Python Functions You Don’t Know About (Probably)</h3></a></div><div class="n o hi"><div class="cd r dq"><div class="o n"><div><a href="https://towardsdatascience.com/@radecicdario?source=post_recirc---------0------------------" rel="noopener"><div class="bj abf abg"><div class="ho n hp o p s hq hr hs ht hu fa"><svg width="49" height="49" viewBox="0 0 49 49"><path fill-rule="evenodd" clip-rule="evenodd" d="M24.5 1.1c-9.39 0-17.53 5.55-21.51 13.66L2 14.28C6.15 5.82 14.66 0 24.5 0S42.85 5.82 47 14.28l-.99.48C42.03 6.65 33.9 1.1 24.5 1.1zM2.99 34.24C6.97 42.35 15.1 47.9 24.5 47.9c9.39 0 17.53-5.55 21.51-13.66l.99.48C42.85 43.18 34.34 49 24.5 49S6.15 43.18 2 34.72l.99-.48z"></path></svg></div><img alt="Dario Radečić" class="r df abg abf" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/2VmdbajrpX9nwOc9UtkV3Yg.png" width="40" height="40"></div></a></div><div class="hv ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as r cm q"><div class="en n o hx"><span class="ao ee bh aq bb hy ba hz ia ib cm"><a href="https://towardsdatascience.com/@radecicdario?source=post_recirc---------0------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Dario Radečić</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------0------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="ao b ap aq ar as r at au"><span class="ao ee bh aq bb hy ba hz ia ib at"><div><a class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener" href="https://towardsdatascience.com/top-3-python-functions-you-dont-know-about-probably-978f4be1e6d?source=post_recirc---------0------------------">Mar 14</a> · 4 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="pq r bj"><div class=""><button class="bt pr ps pt pu pv pw tq iv abh abi"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="pz r"><div class="qa"><h4 class="ao ee bh aq at">2.6K </h4></div></div></div><div class="abj hv cd dy abk r"></div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="35" aria-labelledby="35"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="yj yk yl jl ym yn yo jn yp yq yr jp ys yt yu yv yw yx yy yz za"><div class="ai fm"><div class="r zb"><div class="zc zd xu xv xw ze zf xx xy xz zg zh ya yb yc zi zj yd ye yf zk zl yg yh yi n qc"><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="zu r uf f"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><div class="ax r zv xr"><a class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb r" rel="noopener" href="https://towardsdatascience.com/i-thought-i-was-mastering-python-until-i-discovered-these-tricks-e40d9c71f4e2?source=post_recirc---------1------------------"><div class="zw bj"><div class="fm s ai"><div class="abl r zy zz fm ai aba xl"></div></div></div></a></div></div><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="ax r"><div class="abb cg h abc"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/i-thought-i-was-mastering-python-until-i-discovered-these-tricks-e40d9c71f4e2?source=post_recirc---------1------------------"><h3 class="cm q fz vl ap kq abd abe">I Thought I Was Mastering Python Until I Discovered These Tricks</h3></a></div><div class="n o hi"><div class="cd r dq"><div class="o n"><div><a href="https://towardsdatascience.com/@chouhbik?source=post_recirc---------1------------------" rel="noopener"><img alt="Kamal Chouhbi" class="r df abg abf" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/2dOYKOKixC3TvWiKndwLu4w.jpeg" width="40" height="40"></a></div><div class="hv ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as r cm q"><div class="en n o hx"><span class="ao ee bh aq bb hy ba hz ia ib cm"><a href="https://towardsdatascience.com/@chouhbik?source=post_recirc---------1------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Kamal Chouhbi</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------1------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="ao b ap aq ar as r at au"><span class="ao ee bh aq bb hy ba hz ia ib at"><div><a class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener" href="https://towardsdatascience.com/i-thought-i-was-mastering-python-until-i-discovered-these-tricks-e40d9c71f4e2?source=post_recirc---------1------------------">Mar 6</a> · 9 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="pq r bj"><div class=""><button class="bt pr ps pt pu pv pw tq iv abh abi"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="pz r"><div class="qa"><h4 class="ao ee bh aq at">3.7K </h4></div></div></div><div class="abj hv cd dy abk r"></div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="36" aria-labelledby="36"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="yj yk yl jl ym yn yo jn yp yq yr jp ys yt yu yv yw yx yy yz za"><div class="ai fm"><div class="r zb"><div class="zc zd xu xv xw ze zf xx xy xz zg zh ya yb yc zi zj yd ye yf zk zl yg yh yi n qc"><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="zu r uf f"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><div class="ax r zv xr"><a class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb r" rel="noopener" href="https://towardsdatascience.com/9-reasons-why-youll-never-become-a-data-scientist-c8c5b75503cf?source=post_recirc---------2------------------"><div class="zw bj"><div class="fm s ai"><div class="abm r zy zz fm ai aba xl"></div></div></div></a></div></div><div class="yj yk yl jl ym yn zm zn yp yq zo zp ys yt zq zr yw yx zs zt za"><div class="ax r"><div class="abb cg h abc"><h4 class="ao ee bh aq at">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/9-reasons-why-youll-never-become-a-data-scientist-c8c5b75503cf?source=post_recirc---------2------------------"><h3 class="cm q fz vl ap kq abd abe">9 Reasons why you’ll never become a Data Scientist</h3></a></div><div class="n o hi"><div class="cd r dq"><div class="o n"><div><a href="https://towardsdatascience.com/@rheamoutafis?source=post_recirc---------2------------------" rel="noopener"><div class="bj abf abg"><div class="ho n hp o p s hq hr hs ht hu fa"><svg width="49" height="49" viewBox="0 0 49 49"><path fill-rule="evenodd" clip-rule="evenodd" d="M24.5 1.1c-9.39 0-17.53 5.55-21.51 13.66L2 14.28C6.15 5.82 14.66 0 24.5 0S42.85 5.82 47 14.28l-.99.48C42.03 6.65 33.9 1.1 24.5 1.1zM2.99 34.24C6.97 42.35 15.1 47.9 24.5 47.9c9.39 0 17.53-5.55 21.51-13.66l.99.48C42.85 43.18 34.34 49 24.5 49S6.15 43.18 2 34.72l.99-.48z"></path></svg></div><img alt="Rhea Moutafis" class="r df abg abf" src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/28TyJoz2oyfux06hOJujerQ.jpeg" width="40" height="40"></div></a></div><div class="hv ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as r cm q"><div class="en n o hx"><span class="ao ee bh aq bb hy ba hz ia ib cm"><a href="https://towardsdatascience.com/@rheamoutafis?source=post_recirc---------2------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Rhea Moutafis</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="ao b ap aq ar as r at au"><span class="ao ee bh aq bb hy ba hz ia ib at"><div><a class="bm bn bo bp bq br bs bt bu bv ic by bz ca cb" rel="noopener" href="https://towardsdatascience.com/9-reasons-why-youll-never-become-a-data-scientist-c8c5b75503cf?source=post_recirc---------2------------------">Mar 9</a> · 7 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="pq r bj"><div class=""><button class="bt pr ps pt pu pv pw tq iv abh abi"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="pz r"><div class="qa"><h4 class="ao ee bh aq at">2.8K </h4></div></div></div><div class="abj hv cd dy abk r"></div><div class="iv"><div><div class="bd" role="tooltip" aria-hidden="true" aria-describedby="37" aria-labelledby="37"><button class="bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="sc r sd se"><section class="et eu ai db r sf sg sh si sj sk sl sm sn so sp sq sr ss st"><div class="su sv rg n hi g"><div class="sw n hi"><div class="sx r sy"><div class="sz r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ta tb by bz tc td" rel="noopener"><h4 class="te tf tg ao la ap ru th ti r">Discover <!-- -->Medium</h4></a></div><span class="ao b ap aq ar as r tj tk">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv by bz tc td tl" rel="noopener">Watch</a></span></div><div class="sx r sy"><div class="tm r"><a href="https://medium.com/topics?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ta tb by bz tc td" rel="noopener"><h4 class="te tf tg ao la ap ru th ti r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="ao b ap aq ar as r tj tk">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv by bz tc td tl" rel="noopener">Explore</a></span></div><div class="sx r sy"><div class="sz r"><a href="https://medium.com/membership?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ta tb by bz tc td" rel="noopener"><h4 class="te tf tg ao la ap ru th ti r">Become a member</h4></a></div><span class="ao b ap aq ar as r tj tk">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv by bz tc td tl" rel="noopener">Upgrade</a></span></div></div></div><div class="n o hi"><a href="https://medium.com/?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ta tb by bz tc td" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="tf"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="ao b ap aq ar as r tj tk"><div class="tn to n hi tp al"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz tc td" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz tc td" rel="noopener">Help</a><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----210532383785----------------------" class="bm bn bo bp bq br bs bt bu bv ic by bz tc td" rel="noopener">Legal</a></div></span></div></section></div></div></div><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/16180790160.js"></script><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/a16180790160.html" tabindex="-1" title="Optimizely Internal Frame" style="display: none;" width="0" hidden="" height="0"></iframe><script>window.__BUILD_ID__ = "master-20200318-024810-9691c0e479"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200318-024810-9691c0e479","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200318-024810-9691c0e479"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"context":{"deployment":{"target":"production","tag":"master-20200318-024810-9691c0e479","commit":"9691c0e479e9d137fb71bc746017f0d3b0bee84d"}}},"sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["0000","0001","0002","0003"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}}},"debug":{"requestId":"66695c3f-e878-4881-a8fe-fc7023b08174","originalSpanCarrier":{"ot-tracer-spanid":"371bce265b459026","ot-tracer-traceid":"5b2ab36a34871a23","ot-tracer-sampled":"true"}},"session":{"user":{"id":"6337a822391e"},"xsrf":"vN7yJuupBF9N"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":true},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}},"postRead":false},"client":{"isBot":false,"isEu":false,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":true},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_social_buttons_on_top","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_apple_grace_period","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_daily_read_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_digest_feature_logging","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_free_corona_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_grace_period_google_play","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.42":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_lo_meter_swap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_minimal_meter_v2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_monthly_membership_default","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_mute","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_pardon_the_interruption_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_post_page_nav_stickiness_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_recaptcha_v3","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_sign_up_with_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"enable_ticks_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.97":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"iceland_home_page_loadtest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.100":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.103":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.104":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.105":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.105.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.105.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.106":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.106.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.106.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.107":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.107.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.107.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.108":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.108.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.108.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.105","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.106","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.107","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.108","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:6337a822391e","typename":"User"},"meterPost({\"postId\":\"210532383785\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"210532383785\"})":{"type":"id","generated":false,"id":"Post:210532383785","typename":"Post"}},"User:6337a822391e":{"id":"6337a822391e","username":"monty991999","name":"Mrityunjay Bhardwaj","imageId":"0*OXpS9g7Ercq41VTs.","mediumMemberAt":0,"hasPastMemberships":true,"isPartnerProgramEnrolled":false,"email":"monty991999@gmail.com","unverifiedEmail":"","createdAt":1514394075923,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":["49eb5a30e242","e15c385685b8","210532383785"]},"maxUnlockCount":3,"unlocksRemaining":0},"Post:210532383785":{"__typename":"Post","id":"210532383785","visibility":"LOCKED","latestPublishedVersion":"faa57f4fb80a","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"creator":{"type":"id","generated":false,"id":"User:1a05d06f496d","typename":"User"},"isLocked":true,"lockedSource":"LOCKED_POST_SOURCE_UGC","sequence":null,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1549299606153,"isPublished":true,"layerCake":3,"primaryTopic":{"type":"id","generated":false,"id":"artificial-intelligence","typename":"Topic"},"title":"Audio AI: isolating vocals from stereo music using Convolutional Neural Networks","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":14.028301886792452,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":3408,"viewerClapCount":null,"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:signal-processing","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:audio","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:music","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:towards-data-science","typename":"Tag"}],"topics":[{"type":"id","generated":false,"id":"artificial-intelligence","typename":"Topic"},{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"},{"type":"id","generated":false,"id":"data-science","typename":"Topic"}],"voterCount":571,"recommenders":[],"postResponses":{"type":"id","generated":true,"id":"$Post:210532383785.postResponses","typename":"PostResponses"},"responsesCount":55,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1573524988151,"previewContent":{"type":"id","generated":true,"id":"$Post:210532383785.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*TWabaG5N1WaKuALbr8lGHw.png","typename":"ImageMetadata"},"updatedAt":1573524988406,"seoDescription":"","isSuspended":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","domain":"towardsdatascience.com","slug":"towards-data-science","__typename":"Collection","isAuroraVisible":false,"googleAnalyticsId":null,"customStyleSheet":null,"colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"},"name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"viewerIsMuting":false,"description":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":true,"viewerIsSubscribedToLetters":false,"isUserSubscribedToCollectionEmails":true,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"tagline":"A Medium publication sharing concepts, ideas, and codes."},"User:1a05d06f496d":{"id":"1a05d06f496d","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Ale Koretzky","isFollowing":false,"username":"ale.koretzky","bio":"Head of Machine Learning @Splice. Undercover biz-product guy. Advisor. Made in 🇦🇷, living in Venice, CA.","imageId":"1*RsCKqWhLuOLSzBRiBYEVow.jpeg","mediumMemberAt":1553818367000,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":""},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Picks","url":"https:\u002F\u002Ftowardsdatascience.com\u002Four-picks\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"More","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmore\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"artificial-intelligence":{"name":"Artificial Intelligence","slug":"artificial-intelligence","__typename":"Topic","isFollowing":null},"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"9fde","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1":{"name":"a31b","startIndex":12,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.2":{"name":"0142","startIndex":13,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.1","typename":"Section"},{"type":"id","generated":true,"id":"$Post:210532383785.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.2","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_97","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_98","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_99","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_100","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_101","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_102","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_103","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_104","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_105","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_106","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_107","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_108","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_109","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_110","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_111","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_112","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_113","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_114","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_115","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:faa57f4fb80a_116","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:faa57f4fb80a_0":{"id":"faa57f4fb80a_0","name":"3f29","type":"IMG","href":null,"layout":"FULL_WIDTH","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TWabaG5N1WaKuALbr8lGHw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TWabaG5N1WaKuALbr8lGHw.png":{"id":"1*TWabaG5N1WaKuALbr8lGHw.png","originalHeight":1050,"originalWidth":3268,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_1":{"id":"faa57f4fb80a_1","name":"2555","type":"H3","href":null,"layout":null,"metadata":null,"text":"Audio AI: isolating vocals from stereo music using Convolutional Neural Networks","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_2":{"id":"faa57f4fb80a_2","name":"c145","type":"H4","href":null,"layout":null,"metadata":null,"text":"hacking music towards the democratization of derivative content","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_3":{"id":"faa57f4fb80a_3","name":"95b7","type":"P","href":null,"layout":null,"metadata":null,"text":"What if we could go back to 1965, knock on Abbey Road Studios’ front door holding an ‘All Access’ badge, and have the privilege of listening to those signature Lennon-McCartney harmonies A-Capella? Our input here is a medium quality mp3 of We Can Work it Out by The Beatles. The top track is the input mix and the bottom track, the isolated vocals coming out of our model.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_3.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_3.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_3.markups.0":{"type":"EM","start":240,"end":259,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_3.markups.1":{"type":"EM","start":261,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_4":{"id":"faa57f4fb80a_4","name":"8cb8","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_4.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:33efc65a5493f41ed446e83038d4f0ff":{"id":"33efc65a5493f41ed446e83038d4f0ff","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F305275806%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F305275806&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F744738408_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":787,"iframeWidth":1920,"title":"vocal isolation from stereo using Convolutional Neural Networks","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_4.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:33efc65a5493f41ed446e83038d4f0ff","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_5":{"id":"faa57f4fb80a_5","name":"5803","type":"P","href":null,"layout":null,"metadata":null,"text":"Formally known as Audio Source Separation, the problem we are trying to solve here consists in recovering or reconstructing one or more source signals that, through some -linear or convolutive- process, have been mixed with other signals. The field has many practical applications including but not limited to speech denoising and enhancement, music remixing, spatial audio, remastering, etc. In the context of music production, it is sometimes referred to as unmixing or demixing. There’s a good amount of resources on the subject, going from ICA-based -blind- Source Separation, to semi-supervised Non-negative Matrix Factorization techniques, to more recent neural network-based approaches. For a nice walkthrough on the first two, you can check out these tutorial mini-series from CCRMA, which I found very useful back in the day.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_5.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_5.markups.0":{"type":"A","start":753,"end":779,"href":"https:\u002F\u002Fccrma.stanford.edu\u002F~njb\u002Fteaching\u002Fsstutorial\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.1":{"type":"EM","start":18,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.2":{"type":"EM","start":135,"end":150,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.3":{"type":"EM","start":171,"end":177,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.4":{"type":"EM","start":178,"end":180,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.5":{"type":"EM","start":181,"end":193,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.6":{"type":"EM","start":460,"end":468,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.7":{"type":"EM","start":472,"end":480,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_5.markups.8":{"type":"EM","start":555,"end":561,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_6":{"id":"faa57f4fb80a_6","name":"c191","type":"P","href":null,"layout":null,"metadata":null,"text":"But before jumping into design stuff.. a liiittle bit of Applied Machine Learning philosophy…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_6.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_6.markups.0":{"type":"STRONG","start":0,"end":93,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_7":{"id":"faa57f4fb80a_7","name":"2990","type":"P","href":null,"layout":null,"metadata":null,"text":"As someone who’s been working in signal & image processing for a while and prior to the ‘deep-learning-solves-it-all’ boom, I will introduce the solution as a Feature Engineering journey and show you why, for this particular problem, an artificial neural network ends up being the best approach. Why? Very often I find people writing things like:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_7.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_7.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_7.markups.0":{"type":"STRONG","start":159,"end":178,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_7.markups.1":{"type":"STRONG","start":200,"end":232,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_7.markups.2":{"type":"EM","start":89,"end":118,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_8":{"id":"faa57f4fb80a_8","name":"8756","type":"P","href":null,"layout":null,"metadata":null,"text":"“with deep learning you don’t have to worry about feature engineering anymore; it does it for you”","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_8.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_8.markups.0":{"type":"EM","start":0,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_9":{"id":"faa57f4fb80a_9","name":"f189","type":"P","href":null,"layout":null,"metadata":null,"text":"or worst…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_9.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_9.markups.0":{"type":"EM","start":2,"end":3,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_10":{"id":"faa57f4fb80a_10","name":"7c59","type":"P","href":null,"layout":null,"metadata":null,"text":"“the difference between machine learning and deep learning \u003C let me stop you right there… Deep Learning is still Machine Learning! \u003E is that in ML you do the feature extraction and in deep learning it happens automatically inside the network”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_10.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_10.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_10.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_10.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_10.markups.0":{"type":"STRONG","start":59,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_10.markups.1":{"type":"STRONG","start":131,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_10.markups.2":{"type":"EM","start":0,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_10.markups.3":{"type":"EM","start":133,"end":243,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_11":{"id":"faa57f4fb80a_11","name":"e05c","type":"P","href":null,"layout":null,"metadata":null,"text":"These generalizations, probably coming from the fact that DNNs can be pretty effective at learning good latent spaces, are just wrong. It frustrates me to see recent grads and practitioners being sold on the above misconceptions and going for the ‘deep-learning-solves-it-all’ approach as something you just throw a bunch of raw data at (yes, even after doing some pre-processing you can still be a sinner :)), and expect things to just work as desired. In the real world, where your data is not as simple, clean and pretty as the MNIST dataset and where you have to care about things like real-time, memory and so on, these misconceptions can leave you stuck in experimentation mode for a very long time…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_11.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_11.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_11.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_11.markups.0":{"type":"EM","start":248,"end":277,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_11.markups.1":{"type":"EM","start":285,"end":286,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_11.markups.2":{"type":"EM","start":288,"end":289,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_12":{"id":"faa57f4fb80a_12","name":"b56c","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Feature Engineering not only remains a very important discipline when designing artificial neural networks; in most cases and just like with any other ML technique, it separates production-ready solutions from failing or underperforming experiments. A deep understanding of your data and its domain can still get you very far…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_12.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_12.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_12.markups.0":{"type":"STRONG","start":0,"end":178,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_12.markups.1":{"type":"STRONG","start":210,"end":326,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_13":{"id":"faa57f4fb80a_13","name":"fe11","type":"H3","href":null,"layout":null,"metadata":null,"text":"From A to Z","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_14":{"id":"faa57f4fb80a_14","name":"9fcc","type":"P","href":null,"layout":null,"metadata":null,"text":"Ok, now that I’m done preaching, let’s get into what you came for! Just like with every other data problem that I’ve worked on in my career, I’ll begin by asking the question “how does the data look like”?. Let’s take a look at the following fragment of singing voice from an original studio recording.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_14.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_14.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_14.markups.0":{"type":"STRONG","start":175,"end":205,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_14.markups.1":{"type":"EM","start":175,"end":205,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_15":{"id":"faa57f4fb80a_15","name":"db4b","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"‘One Last Time’ studio vocals, Ariana Grande","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_15.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:12a11389eb8f44eccdaf3fb1f2444a86":{"id":"12a11389eb8f44eccdaf3fb1f2444a86","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F305288385%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F305288385&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F744755870_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":787,"iframeWidth":1920,"title":"Studio vocals, Ariana Grande","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_15.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:12a11389eb8f44eccdaf3fb1f2444a86","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_16":{"id":"faa57f4fb80a_16","name":"c6cf","type":"P","href":null,"layout":null,"metadata":null,"text":"Not too interesting right? Well, this is because we are visualizing the waveform or time-domain signal, where all we have access to are the amplitude values of the signal over time. We could extract things such as envelopes, RMS values, zero-crossing rate, etc, but these features are too primitive and not discriminative enough for helping us solve the problem. If we want to extract vocal content from a mix we should somehow expose the structure of human speech, to begin with. Luckily, the Short-Time Fourier Transform (STFT) comes to the rescue.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_16.markups.7","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_16.markups.0":{"type":"A","start":494,"end":529,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FShort-time_Fourier_transform","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.1":{"type":"STRONG","start":428,"end":434,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.2":{"type":"STRONG","start":435,"end":466,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.3":{"type":"STRONG","start":479,"end":481,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.4":{"type":"EM","start":84,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.5":{"type":"EM","start":272,"end":280,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.6":{"type":"EM","start":285,"end":288,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_16.markups.7":{"type":"EM","start":289,"end":299,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_17":{"id":"faa57f4fb80a_17","name":"f0c2","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"STFT magnitude spectrum - window size = 2048, overlap = 75%, log-frequency [Sonic Visualizer]","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_17.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:75f41ef20631c41827844dbd2100b11d":{"id":"75f41ef20631c41827844dbd2100b11d","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F305391461%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F305391461&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F744889332_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":987,"iframeWidth":1920,"title":"Studio vocals, Ariana Grande - STFT","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_17.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:75f41ef20631c41827844dbd2100b11d","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_18":{"id":"faa57f4fb80a_18","name":"565c","type":"P","href":null,"layout":null,"metadata":null,"text":"Although I love Speech Processing and I would definitely enjoy going through source-filter modeling, cepstrum, quefrencies, LPC, MFCC and so on, I’ll skip all that stuff and focus on the core elements related to our problem, so that the article is digestible by as many people as possible and not exclusively by the Audio Signal Processing \u002F Speech community.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_18.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_18.markups.0":{"type":"EM","start":77,"end":133,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_19":{"id":"faa57f4fb80a_19","name":"af89","type":"P","href":null,"layout":null,"metadata":null,"text":"So, what does the structure of human speech tell us?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_20":{"id":"faa57f4fb80a_20","name":"7af5","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*zH6VF4UG4xw6s6XWjqz2pg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zH6VF4UG4xw6s6XWjqz2pg.png":{"id":"1*zH6VF4UG4xw6s6XWjqz2pg.png","originalHeight":1452,"originalWidth":2862,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_21":{"id":"faa57f4fb80a_21","name":"03d2","type":"P","href":null,"layout":null,"metadata":null,"text":"Well, there are 3 main elements that we can identify here:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_22":{"id":"faa57f4fb80a_22","name":"ae69","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A fundamental frequency (f0), determined by the frequency of vibration of our vocal cords. In this case, Ariana is singing in the 300–500 Hz range.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_22.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_22.markups.0":{"type":"STRONG","start":2,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_23":{"id":"faa57f4fb80a_23","name":"a104","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A number of harmonics above f0, following a similar shape or pattern. These harmonics happen at integer multiples of f0.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_23.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_23.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_23.markups.0":{"type":"STRONG","start":12,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_23.markups.1":{"type":"EM","start":52,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_24":{"id":"faa57f4fb80a_24","name":"71c5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"unvoiced speech, which includes consonants like ‘t’, ‘p’, ‘k’, ‘s’ (which are not produced by the vibration of our vocal cords), breaths, etc. They manifest as short bursts in the high-frequency region.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_24.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_24.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_24.markups.0":{"type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_24.markups.1":{"type":"EM","start":47,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_25":{"id":"faa57f4fb80a_25","name":"f8fa","type":"H3","href":null,"layout":null,"metadata":null,"text":"A first shot using a rule-based approach","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_25.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_25.markups.0":{"type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_26":{"id":"faa57f4fb80a_26","name":"c182","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s forget for a second about this thing called Machine Learning. Based on our knowledge about the data, can we come up with a method to extract our vocals? Let me give it a try…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_27":{"id":"faa57f4fb80a_27","name":"d7ed","type":"P","href":null,"layout":null,"metadata":null,"text":"Naive vocal isolation V1.0:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_27.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_27.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_27.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_27.markups.1":{"type":"EM","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_28":{"id":"faa57f4fb80a_28","name":"d663","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Identify vocal sections. There’s a lot going on within a mix. We want to focus on the sections that actually contain vocal content and ignore the rest.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_29":{"id":"faa57f4fb80a_29","name":"9d5d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Distinguish between voiced and unvoiced sections. As we saw, voiced speech looks very different from unvoiced speech, therefore they probably need different treatment.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_30":{"id":"faa57f4fb80a_30","name":"6994","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Estimate the frequency of the fundamental over time.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_31":{"id":"faa57f4fb80a_31","name":"3137","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Based on the output of 3, apply some sort of mask to capture the harmonic content.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_32":{"id":"faa57f4fb80a_32","name":"0cc7","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Do something else about the unvoiced sections…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_33":{"id":"faa57f4fb80a_33","name":"082b","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*V30mUf8OHZNaWrHxluUbEg.gif","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*V30mUf8OHZNaWrHxluUbEg.gif":{"id":"1*V30mUf8OHZNaWrHxluUbEg.gif","originalHeight":603,"originalWidth":1050,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_34":{"id":"faa57f4fb80a_34","name":"97cf","type":"P","href":null,"layout":null,"metadata":null,"text":"If we do a decent job, the output of this process should be a soft or binary mask that, when applied (element-wise multiplication) to the magnitude STFT of the mix, gives us an approximate reconstruction of the magnitude STFT of the vocals. From there, we then combine this vocal STFT estimate with the phase information of the original mix, compute an inverse STFT, and obtain the time-domain signal of the reconstructed vocals.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_34.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_34.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_34.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_34.markups.0":{"type":"EM","start":62,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_34.markups.1":{"type":"EM","start":70,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_34.markups.2":{"type":"EM","start":77,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_35":{"id":"faa57f4fb80a_35","name":"2cf1","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*bKZbYE0YQ2u4SpgJlHvJnA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*bKZbYE0YQ2u4SpgJlHvJnA.png":{"id":"1*bKZbYE0YQ2u4SpgJlHvJnA.png","originalHeight":1445,"originalWidth":1805,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_36":{"id":"faa57f4fb80a_36","name":"d495","type":"P","href":null,"layout":null,"metadata":null,"text":"Doing this from scratch is already a lot of work. But for the sake of the demonstration, we’re going to use an implementation of the pYIN algorithm. Even though it’s meant for solving step 3, with the right constraints it takes care of 1 and 2 pretty decently, while tracking the vocal fundamental even in the presence of music. The example below contains the output from this approach, without addressing the unvoiced sections.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_36.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_36.markups.0":{"type":"A","start":133,"end":147,"href":"https:\u002F\u002Fcode.soundsoftware.ac.uk\u002Fprojects\u002Fpyin","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_37":{"id":"faa57f4fb80a_37","name":"4298","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_37.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:f2ca26ad834e0d5a7ec07d9fcf12d165":{"id":"f2ca26ad834e0d5a7ec07d9fcf12d165","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F305636014%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F305636014&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F745195024_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":849,"iframeWidth":1920,"title":"ariana harmonic mask","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_37.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:f2ca26ad834e0d5a7ec07d9fcf12d165","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_38":{"id":"faa57f4fb80a_38","name":"cba7","type":"P","href":null,"layout":null,"metadata":null,"text":"Well…? It sort of did the trick but the quality of the recovered vocal is not there yet. Maybe with additional time, energy and budget we can improve this method and get it to a better place.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_39":{"id":"faa57f4fb80a_39","name":"8701","type":"P","href":null,"layout":null,"metadata":null,"text":"Now let me ask you…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_40":{"id":"faa57f4fb80a_40","name":"c371","type":"P","href":null,"layout":null,"metadata":null,"text":"What happens when you have multiple vocals, which is definitely the case in at least 50% of today’s professionally produced tracks?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_40.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_40.markups.0":{"type":"STRONG","start":27,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_41":{"id":"faa57f4fb80a_41","name":"c427","type":"P","href":null,"layout":null,"metadata":null,"text":"What happens when the vocals have been processed with reverberation, delays and other effects? Let’s take a look at the last chorus of Ariana Grande’s One Last Time.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_41.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_41.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_41.markups.0":{"type":"STRONG","start":54,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_41.markups.1":{"type":"STRONG","start":69,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_41.markups.2":{"type":"EM","start":151,"end":165,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_42":{"id":"faa57f4fb80a_42","name":"50fe","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_42.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:810901d388966b6cc7fe2401a6cbf539":{"id":"810901d388966b6cc7fe2401a6cbf539","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F306589126%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F306589126&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F746366582_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":995,"iframeWidth":1920,"title":"Ariana Grande 'One Last Time' - Multiple vocals (last chorus)","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_42.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:810901d388966b6cc7fe2401a6cbf539","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_43":{"id":"faa57f4fb80a_43","name":"81fc","type":"P","href":null,"layout":null,"metadata":null,"text":"Are you feeling the pain already…? I am.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_44":{"id":"faa57f4fb80a_44","name":"d8f5","type":"P","href":null,"layout":null,"metadata":null,"text":"Very soon, ad-hoc methods like the one described above become a house of cards. The problem is just too complex. There are too many rules, too many exceptions to the rules and too many varying conditions (effects and different mixing settings). The multi-step approach also implies that errors in one step propagate issues to the step that comes after. Improving each step would be very costly, it would require a large number of iterations to get right and last but not least, we would probably end up with a computationally expensive pipeline, something that by itself can be a deal-breaker.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_45":{"id":"faa57f4fb80a_45","name":"28f2","type":"P","href":null,"layout":null,"metadata":null,"text":"These are the kind of scenarios where we need to start thinking of a more end-to-end approach and let ML figure out -PART- of the underlying processes and operations required to solve the problem. However, we are not throwing the towel when it comes to feature engineering and you’ll see why.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_45.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_45.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_45.markups.0":{"type":"STRONG","start":0,"end":292,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_45.markups.1":{"type":"EM","start":74,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_46":{"id":"faa57f4fb80a_46","name":"854b","type":"H3","href":null,"layout":null,"metadata":null,"text":"The hypothesis: we can use a CNN as a transfer function that maps mixes into vocals","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_47":{"id":"faa57f4fb80a_47","name":"2261","type":"P","href":null,"layout":null,"metadata":null,"text":"Inspired by the achievements with CNNs on natural images, why not apply the same reasoning here?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_48":{"id":"faa57f4fb80a_48","name":"bc2a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*L86EVDCdz3HQ5NpxfPWLTg.png","typename":"ImageMetadata"},"text":"CNNs have been successful at tasks such as image colorization, deblurring and super-resolution.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*L86EVDCdz3HQ5NpxfPWLTg.png":{"id":"1*L86EVDCdz3HQ5NpxfPWLTg.png","originalHeight":1070,"originalWidth":1070,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_49":{"id":"faa57f4fb80a_49","name":"fa1e","type":"P","href":null,"layout":null,"metadata":null,"text":"At the end of the day, we know we can represent an audio signal ‘as an image’ using the Short-Time Fourier Transform right? Even though these audio images don’t follow the statistical distribution of natural images, they still expose spatial patterns (in the time vs frequency space) that we should be able to learn from.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_49.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_49.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_49.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_49.markups.0":{"type":"EM","start":76,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_49.markups.1":{"type":"EM","start":142,"end":147,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_49.markups.2":{"type":"EM","start":148,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_50":{"id":"faa57f4fb80a_50","name":"d0cc","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*X0nGpE5CkQgYJnINcALD_w.png","typename":"ImageMetadata"},"text":"Mix: you can see the kick drum and baseline at the bottom, and some of the synths in the middle getting mixed with the vocals. On the right, the corresponding vocals-only","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*X0nGpE5CkQgYJnINcALD_w.png":{"id":"1*X0nGpE5CkQgYJnINcALD_w.png","originalHeight":992,"originalWidth":1706,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_51":{"id":"faa57f4fb80a_51","name":"78bc","type":"P","href":null,"layout":null,"metadata":null,"text":"At the time, validating this experiment was a costly endeavor, because obtaining or generating the training data required was already a big investment. One of the practices I always try to implement in applied research is to first identify a simpler problem that validates the same principles as the original one, but that does not require as much work. This allows you to keep your hypotheses smaller, iterate faster and pivot with minimum impact when things don’t work as expected.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_51.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_51.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_51.markups.0":{"type":"STRONG","start":231,"end":292,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_51.markups.1":{"type":"STRONG","start":293,"end":314,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_52":{"id":"faa57f4fb80a_52","name":"1105","type":"P","href":null,"layout":null,"metadata":null,"text":"An implied condition for the original solution to work is that a CNN must be capable of understanding the structure of human speech. A simpler problem can then be: given a mix fragment, let’s see if a CNN can classify these fragments as containing vocal content or not. We are looking at a music-robust Vocal Activity Detector (VAD), implemented as a binary classifier.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_52.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_52.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_52.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_52.markups.0":{"type":"A","start":303,"end":332,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FVoice_activity_detection","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_52.markups.1":{"type":"STRONG","start":65,"end":131,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_52.markups.2":{"type":"EM","start":164,"end":268,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_53":{"id":"faa57f4fb80a_53","name":"2719","type":"H4","href":null,"layout":null,"metadata":null,"text":"Designing our feature space","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_54":{"id":"faa57f4fb80a_54","name":"0b3d","type":"P","href":null,"layout":null,"metadata":null,"text":"We know that audio signals such as music and human speech embed temporal dependencies. In simpler terms, nothing happens in isolation at a given time-frame. If I want to know whether a given section of audio contains human speech or not, I should probably look at the neighbor regions as well. That temporal context can give me good information about what’s going on in the region of interest. At the same time, we want to perform our classification in very small time increments, so that we can capture the human voice with the highest temporal resolution possible.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_54.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_54.markups.0":{"type":"EM","start":299,"end":315,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_55":{"id":"faa57f4fb80a_55","name":"641b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4ZoIG-CYTxz6nTtvVlZv2g.gif","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4ZoIG-CYTxz6nTtvVlZv2g.gif":{"id":"1*4ZoIG-CYTxz6nTtvVlZv2g.gif","originalHeight":684,"originalWidth":1296,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_56":{"id":"faa57f4fb80a_56","name":"1572","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s do some numbers…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_57":{"id":"faa57f4fb80a_57","name":"a93d","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Sampling rate (fs): 22050 Hz (we are downsampling from 44100 to 22050)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_58":{"id":"faa57f4fb80a_58","name":"ea4b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"STFT design: window size = 1024, hop size = 256, Mel scale interpolation for perceptual weighting. Since our input data is real, we can work with one half of the STFT (the why is out of the scope of this post…) while keeping the DC component (not a requirement), giving us 513 frequency bins.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_58.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_58.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_58.markups.0":{"type":"A","start":49,"end":59,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMel_scale","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_58.markups.1":{"type":"EM","start":123,"end":129,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_59":{"id":"faa57f4fb80a_59","name":"e9cc","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Target classification resolution: a single STFT frame (~11.6 milliseconds = 256 \u002F 22050)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_60":{"id":"faa57f4fb80a_60","name":"bbd5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Target temporal context: ~300 milliseconds = 25 STFT frames.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_61":{"id":"faa57f4fb80a_61","name":"a9c9","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Target number of training examples: 0.5M","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_62":{"id":"faa57f4fb80a_62","name":"9023","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Assuming we are using a sliding window with a stride of 1 STFT time-frame to generate our training data, we need around 1.6 hours of labeled audio to generate our 0.5M data samples. [if you’d like to know more details about generating the actual dataset, please feel free to ask in the comments]","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_63":{"id":"faa57f4fb80a_63","name":"0306","type":"P","href":null,"layout":null,"metadata":null,"text":"With the above requirements, the input\u002Foutput data to our binary classifier looks like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_64":{"id":"faa57f4fb80a_64","name":"2d50","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Excr3WeAH5ZUot-RnL0WOA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Excr3WeAH5ZUot-RnL0WOA.png":{"id":"1*Excr3WeAH5ZUot-RnL0WOA.png","originalHeight":674,"originalWidth":1191,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_65":{"id":"faa57f4fb80a_65","name":"8d0a","type":"H4","href":null,"layout":null,"metadata":null,"text":"The model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_65.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_65.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_66":{"id":"faa57f4fb80a_66","name":"2705","type":"P","href":null,"layout":null,"metadata":null,"text":"Using Keras, we can build a small CNN model to validate our hypothesis.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_67":{"id":"faa57f4fb80a_67","name":"f912","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.layers.advanced_activations import LeakyReLU","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_68":{"id":"faa57f4fb80a_68","name":"770d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model = Sequential()\nmodel.add(Conv2D(16, (3,3), padding='same', input_shape=(513, 25, 1)))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(16, (3,3), padding='same'))\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(3,3)))\nmodel.add(Dropout(0.25))","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_69":{"id":"faa57f4fb80a_69","name":"ecec","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model.add(Conv2D(16, (3,3), padding='same'))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(16, (3,3), padding='same'))\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(3,3)))\nmodel.add(Dropout(0.25))","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_70":{"id":"faa57f4fb80a_70","name":"7016","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_71":{"id":"faa57f4fb80a_71","name":"5cbc","type":"PRE","href":null,"layout":null,"metadata":null,"text":"sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=['accuracy'])","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_72":{"id":"faa57f4fb80a_72","name":"128a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*mPtHH7QbrxeNPxInt0wM6g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*mPtHH7QbrxeNPxInt0wM6g.png":{"id":"1*mPtHH7QbrxeNPxInt0wM6g.png","originalHeight":1907,"originalWidth":535,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_73":{"id":"faa57f4fb80a_73","name":"8c17","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*HBtu3jtQ8k9vkaYFPDTN3A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*HBtu3jtQ8k9vkaYFPDTN3A.png":{"id":"1*HBtu3jtQ8k9vkaYFPDTN3A.png","originalHeight":1380,"originalWidth":1106,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_74":{"id":"faa57f4fb80a_74","name":"5096","type":"P","href":null,"layout":null,"metadata":null,"text":"With an 80\u002F20 train-test split and after ~50 epochs we reach ~97% test accuracy, which means there’s enough evidence that our CNN model can discriminate between music sections containing vocal content and music sections without vocal content. By inspecting some of the feature maps coming out of our 4th convolutional layer, it looks like our network has optimized its kernels to perform 2 tasks: filtering out music and filtering out vocals…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_74.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_74.markups.0":{"type":"STRONG","start":62,"end":79,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_75":{"id":"faa57f4fb80a_75","name":"a94d","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*C6GmkQuWG-AZoZfR9PNUhg.png","typename":"ImageMetadata"},"text":"Sample feature maps at the output of the 4th conv. layer. Apparently, the output on the left is the result of a combination of kernel operations that try to preserve vocal content while ignoring music. The high values resemble the harmonic structure of human speech. The feature map on the right seems to be the result of the opposite task.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*C6GmkQuWG-AZoZfR9PNUhg.png":{"id":"1*C6GmkQuWG-AZoZfR9PNUhg.png","originalHeight":1154,"originalWidth":1996,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_76":{"id":"faa57f4fb80a_76","name":"bf5a","type":"H3","href":null,"layout":null,"metadata":null,"text":"From VAD to Source Separation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_76.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_76.markups.0":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_77":{"id":"faa57f4fb80a_77","name":"aabb","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we’ve validated this simpler classification problem, how do we go from detecting vocal activity in music all the way to isolating vocals from music? Well, rescuing some ideas from our naive method described at the beginning, we still want to somehow end up with an estimate of the vocal’s magnitude spectrogram. This now becomes a regression problem. What we want to do is, given a time-frame from the STFT of the mix (with enough temporal context), estimate the corresponding vocal time-frame’s magnitude spectrum.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_77.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_77.markups.0":{"type":"EM","start":193,"end":198,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_78":{"id":"faa57f4fb80a_78","name":"2274","type":"P","href":null,"layout":null,"metadata":null,"text":"What about the training set? (you might be asking yourself at this point)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_78.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_78.markups.0":{"type":"STRONG","start":0,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_79":{"id":"faa57f4fb80a_79","name":"d68b","type":"P","href":null,"layout":null,"metadata":null,"text":"oh Lord… that was something. I’m gonna address this at the end of the article so that we don’t switch contexts yet!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_80":{"id":"faa57f4fb80a_80","name":"aff5","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*N_Vy2dGenOEbjRWvgHJ-EA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*N_Vy2dGenOEbjRWvgHJ-EA.png":{"id":"1*N_Vy2dGenOEbjRWvgHJ-EA.png","originalHeight":668,"originalWidth":1138,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_81":{"id":"faa57f4fb80a_81","name":"ea6e","type":"P","href":null,"layout":null,"metadata":null,"text":"If our model learns well, during inference, all we need to do is implement a simple sliding window over the STFT of the mix. After each prediction, we move our window to the right by 1 time-frame, predict the next vocal frame and concatenate it with the previous prediction. In regards to the model, we can start by using the same model we used for VAD as a baseline and by making some changes (output shape is now (513,1), linear activation at the output, MSE as loss function), we can begin our training.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_82":{"id":"faa57f4fb80a_82","name":"8551","type":"P","href":null,"layout":null,"metadata":null,"text":"Don’t claim victory yet…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_82.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_82.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_83":{"id":"faa57f4fb80a_83","name":"73a3","type":"P","href":null,"layout":null,"metadata":null,"text":"Although the above input\u002Foutput representation makes sense, after training our vocal separation model several times, with varying parameters and data normalizations, the results are not there yet. It seems like we are asking for too much…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_83.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_83.markups.0":{"type":"STRONG","start":211,"end":238,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_84":{"id":"faa57f4fb80a_84","name":"5134","type":"P","href":null,"layout":null,"metadata":null,"text":"We went from a binary classifier to trying to do regression on a 513-dimensional vector. Although the network learns the task to a degree, after reconstructing the vocal’s time domain signal, there are obvious artifacts and interferences from other sources. Even after adding more layers and increasing the number of model parameters, the results don’t change much.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_84.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_84.markups.0":{"type":"EM","start":49,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_85":{"id":"faa57f4fb80a_85","name":"6d57","type":"P","href":null,"layout":null,"metadata":null,"text":"So then the question became: can we trick the network into thinking it is solving a simpler problem and still achieve the desired results?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_85.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_85.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_85.markups.0":{"type":"STRONG","start":28,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_85.markups.1":{"type":"EM","start":59,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_86":{"id":"faa57f4fb80a_86","name":"ff6d","type":"P","href":null,"layout":null,"metadata":null,"text":"What if instead of trying to estimate the vocal’s magnitude STFT, we trained the network to learn a binary mask that, when applied to the STFT of the mix, gives us a simplified but perceptually-acceptable-upon-reconstruction estimate of the vocal’s magnitude spectrogram?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_86.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_86.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_86.markups.0":{"type":"STRONG","start":181,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_86.markups.1":{"type":"EM","start":100,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_87":{"id":"faa57f4fb80a_87","name":"7030","type":"P","href":null,"layout":null,"metadata":null,"text":"By experimenting with different heuristics, we came up with a relatively simple (and definitely unorthodox from a Signal Processing perspective) way to extract singing voice from mixes using binary masks. Without going too much into the details, we are going to think of the output as a binary image where, a value of ‘1’ indicates predominant presence of vocal content at a given frequency and timeframe location, and a value of ‘0’ indicates predominant presence of music at the given location. Visually, it looks pretty unattractive, but upon reconstructing the time domain signal, the results are surprisingly good.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_87.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_87.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_87.markups.0":{"type":"STRONG","start":332,"end":370,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_87.markups.1":{"type":"EM","start":294,"end":299,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_88":{"id":"faa57f4fb80a_88","name":"ca89","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*wo5TZTh9cGLCJM96nSWpdA.png","typename":"ImageMetadata"},"text":"Binary masking (or TF masking) techniques have been around for decades and have been proven very effective in specific audio source separation and classification problems. However, most available techniques have been optimized for speech enhancement \u002F recognition applications and fall short when it comes to real-world data such as professionally-produced & mastered music. Our binarization method was specifically designed for this scenario.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wo5TZTh9cGLCJM96nSWpdA.png":{"id":"1*wo5TZTh9cGLCJM96nSWpdA.png","originalHeight":600,"originalWidth":1130,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_89":{"id":"faa57f4fb80a_89","name":"8a5a","type":"P","href":null,"layout":null,"metadata":null,"text":"Our problem now becomes some sort of regression-classification hybrid. We are asking the model to “classify pixels” at the output as vocal or non-vocal, although conceptually (and also in terms of the loss function used -MSE- ), the task is still a regression one.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_89.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_89.markups.0":{"type":"EM","start":114,"end":116,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_90":{"id":"faa57f4fb80a_90","name":"dd5e","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*5IBjtoObD4Ark2KTqF89nw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*5IBjtoObD4Ark2KTqF89nw.png":{"id":"1*5IBjtoObD4Ark2KTqF89nw.png","originalHeight":671,"originalWidth":1138,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_91":{"id":"faa57f4fb80a_91","name":"94cb","type":"P","href":null,"layout":null,"metadata":null,"text":"Although the distinction might not seem relevant to some, it actually makes a huge difference in the model’s ability to learn the assigned task, the second one being way more simple and constrained. At the same time, it allows us to keep our model relatively small in terms of number of parameters considering the complexity of the task, something highly desired for real-time operation, which was a design requirement in this case. After some minor tweaks, the final model looks like this.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_92":{"id":"faa57f4fb80a_92","name":"1be9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ny8Wj6YKPYQ7PVBI6UxOrg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ny8Wj6YKPYQ7PVBI6UxOrg.png":{"id":"1*ny8Wj6YKPYQ7PVBI6UxOrg.png","originalHeight":1907,"originalWidth":535,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_93":{"id":"faa57f4fb80a_93","name":"be20","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*fBErlCqo-nvKUwB7e2g5tg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fBErlCqo-nvKUwB7e2g5tg.png":{"id":"1*fBErlCqo-nvKUwB7e2g5tg.png","originalHeight":1362,"originalWidth":1116,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_94":{"id":"faa57f4fb80a_94","name":"eeef","type":"H4","href":null,"layout":null,"metadata":null,"text":"How do we reconstruct the time-domain signal?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_95":{"id":"faa57f4fb80a_95","name":"1bce","type":"P","href":null,"layout":null,"metadata":null,"text":"Basically, as described in the naive method section. In this case, for every inference pass that we do, we are predicting a single timeframe of the vocals’ binary mask. Again, by implementing a simple sliding window with a stride of one timeframe, we keep estimating and concatenating consecutive timeframes, which end up making up the whole vocal binary mask.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_95.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_95.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_95.markups.0":{"type":"EM","start":31,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_95.markups.1":{"type":"EM","start":37,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_96":{"id":"faa57f4fb80a_96","name":"214d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iD_RQh916DnUqLm0rphjZw.gif","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iD_RQh916DnUqLm0rphjZw.gif":{"id":"1*iD_RQh916DnUqLm0rphjZw.gif","originalHeight":1008,"originalWidth":1008,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_97":{"id":"faa57f4fb80a_97","name":"1688","type":"H4","href":null,"layout":null,"metadata":null,"text":"Creating the training set","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_97.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_97.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_98":{"id":"faa57f4fb80a_98","name":"e797","type":"P","href":null,"layout":null,"metadata":null,"text":"As you know, one of the biggest pain points in supervised Machine Learning (leave aside all those toy examples with available datasets out there) is having the right data (amount and quality) for the particular problem that you’re trying to solve. Based on the input\u002Foutput representations described, in order to train our model, we first needed a significant number of mixes and their corresponding, perfectly aligned and normalized vocal tracks. There’s more than one way to build this dataset and here we used a combination of strategies, ranging from manually creating mix \u003C\u003E vocal pairs with some acapellas found online, to finding RockBand stems, to web-scraping Youtube. Just to give you an idea of what part of this time-consuming and painful process looked like, our “dataset project” involved creating a tool to automatically build mix \u003C\u003E vocal pairs as illustrated below:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_99":{"id":"faa57f4fb80a_99","name":"f568","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*qQiiiSVHYzOuiOr1DU4QxQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qQiiiSVHYzOuiOr1DU4QxQ.png":{"id":"1*qQiiiSVHYzOuiOr1DU4QxQ.png","originalHeight":1222,"originalWidth":2016,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_100":{"id":"faa57f4fb80a_100","name":"596c","type":"P","href":null,"layout":null,"metadata":null,"text":"We knew we needed a good amount of data for the network to learn the transfer function needed to map mixes into vocals. Our final dataset consisted of around 15M examples of ~300-millisecond fragments of mixes and their corresponding vocal binary masks.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_101":{"id":"faa57f4fb80a_101","name":"cb37","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OwMFUHdQb_Cq1gN0qfAwcg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OwMFUHdQb_Cq1gN0qfAwcg.png":{"id":"1*OwMFUHdQb_Cq1gN0qfAwcg.png","originalHeight":1398,"originalWidth":2106,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:faa57f4fb80a_102":{"id":"faa57f4fb80a_102","name":"819c","type":"H4","href":null,"layout":null,"metadata":null,"text":"Pipeline architecture","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_103":{"id":"faa57f4fb80a_103","name":"dbb3","type":"P","href":null,"layout":null,"metadata":null,"text":"Building a Machine Learning model for a given task is only part of the deal. In production environments, we need to think about software architecture, pipelines, and optimization strategies, especially when we’re dealing with real-time.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_104":{"id":"faa57f4fb80a_104","name":"66ed","type":"P","href":null,"layout":null,"metadata":null,"text":"For this particular problem the reconstruction into the time-domain can be done all at once after predicting a full vocal binary mask (offline mode) or, more interestingly, as part of a multithreaded pipeline where we acquire, process, reconstruct and playback in small segments, allowing this to be streaming-friendly, and even capable of delivering real-time deconstruction on music that’s being recorded on the fly, with minimum latency. Given this is a whole topic on its own, I’m going to leave it for another article focused on real-time ML pipelines…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_104.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_104.markups.0":{"type":"STRONG","start":534,"end":556,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_105":{"id":"faa57f4fb80a_105","name":"fabe","type":"H3","href":null,"layout":null,"metadata":null,"text":"I think I’ve covered enough so why don’t we listen to a couple more examples?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_105.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_105.markups.0":{"type":"STRONG","start":0,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_106":{"id":"faa57f4fb80a_106","name":"087c","type":"H4","href":null,"layout":null,"metadata":null,"text":"Daft Punk — Get Lucky (Studio)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_107":{"id":"faa57f4fb80a_107","name":"c82e","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"we can hear some minimal interference from the drums here…","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_107.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:2be93cd324242f03e355b0451c0d3434":{"id":"2be93cd324242f03e355b0451c0d3434","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F315172280%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F315172280&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F756878254_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":634,"iframeWidth":1920,"title":"'Daft Punk - Get Lucky', vocal isolation from stereo using Convolutional Neural Networks","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_107.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:2be93cd324242f03e355b0451c0d3434","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_108":{"id":"faa57f4fb80a_108","name":"159d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Adele — Set Fire to the Rain (live recording!)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_109":{"id":"faa57f4fb80a_109","name":"c292","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"Notice how at the very beginning our model extracts the crowd’s screaming as vocal content :). In this case we have some additional interference from other sources. This being a live recording it kinda makes sense for this extracted vocal not to be as high quality as the previous ones.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_109.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:985e56d5ddad64a7f8fb936b1ef266da":{"id":"985e56d5ddad64a7f8fb936b1ef266da","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F315172388%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F315172388&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F756878383_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":631,"iframeWidth":1920,"title":"'Adele - Set Fire to the Rain', vocal isolation from stereo using Convolutional Neural Networks","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_109.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:985e56d5ddad64a7f8fb936b1ef266da","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_110":{"id":"faa57f4fb80a_110","name":"8bf9","type":"H3","href":null,"layout":null,"metadata":null,"text":"Ok, so there’s ‘one last thing’ …","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_111":{"id":"faa57f4fb80a_111","name":"826f","type":"H3","href":null,"layout":null,"metadata":null,"text":"Given this works for vocals why not apply it to other instruments…?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_112":{"id":"faa57f4fb80a_112","name":"996f","type":"P","href":null,"layout":null,"metadata":null,"text":"This article is extensive enough already but given you’ve made it this far I thought you deserved to see one last demo. With the exact same reasoning for extracting vocal content, we can try to split a stereo track into STEMs (drums, bassline, vocals, others) by making some modifications to our model and of course, by having the appropriate training set :). If you are interested in the technical details for this extension, just leave me some comments. I will consider writing a ‘part 2’ for the Stereo-to-Stems deconstruction case when time allows!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_113":{"id":"faa57f4fb80a_113","name":"ad72","type":"P","href":null,"layout":null,"metadata":null,"text":"*** UPDATE 06\u002F2019***\nPart 2, Stereo-to-Stems deconstruction is here!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_113.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_113.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_113.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_113.markups.0":{"type":"A","start":64,"end":68,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Faudio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_113.markups.1":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_113.markups.2":{"type":"STRONG","start":63,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_114":{"id":"faa57f4fb80a_114","name":"57d5","type":"IFRAME","href":null,"layout":"OUTSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:faa57f4fb80a_114.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:90c87e97d9ae32d63fe5acb887505898":{"id":"90c87e97d9ae32d63fe5acb887505898","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F315173879%3Fapp_id%3D122963&dntp=1&url=https%3A%2F%2Fvimeo.com%2F315173879&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F756880181_1280.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=vimeo","iframeHeight":635,"iframeWidth":1920,"title":"'Daft Punk - Get Lucky' STEMs isolation from stereo using Convolutional Neural Networks","__typename":"MediaResource"},"$Paragraph:faa57f4fb80a_114.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:90c87e97d9ae32d63fe5acb887505898","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:faa57f4fb80a_115":{"id":"faa57f4fb80a_115","name":"ff66","type":"P","href":null,"layout":null,"metadata":null,"text":"Thanks for reading and don’t hesitate in leaving questions. I will keep writing articles on Audio AI so stay tuned! As a final remark, you can see that the actual CNN model we ended up building is not that special. The success of this work has been driven by focusing on the Feature Engineering aspect and by implementing a lean process for hypotheses validations, something I’ll be writing about in the near future!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_115.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:faa57f4fb80a_115.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:faa57f4fb80a_115.markups.0":{"type":"STRONG","start":92,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_115.markups.1":{"type":"STRONG","start":275,"end":294,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:faa57f4fb80a_116":{"id":"faa57f4fb80a_116","name":"b3ac","type":"P","href":null,"layout":null,"metadata":null,"text":"ps: shoutouts to Naveen Rajashekharappa and Karthiek Reddy Bokka for their contributions to this work!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:signal-processing":{"id":"signal-processing","displayTitle":"Signal Processing","__typename":"Tag"},"Tag:audio":{"id":"audio","displayTitle":"Audio","__typename":"Tag"},"Tag:music":{"id":"music","displayTitle":"Music","__typename":"Tag"},"Tag:towards-data-science":{"id":"towards-data-science","displayTitle":"Towards Data Science","__typename":"Tag"},"machine-learning":{"name":"Machine Learning","__typename":"Topic","slug":"machine-learning"},"data-science":{"name":"Data Science","__typename":"Topic","slug":"data-science"},"$Post:210532383785.postResponses":{"count":0,"__typename":"PostResponses","responsesConnection({\"paging\":{\"limit\":10}})":{"type":"id","generated":true,"id":"$Post:210532383785.postResponses.responsesConnection({\"paging\":{\"limit\":10}})","typename":"StreamConnection"}},"$Post:210532383785.previewContent":{"subtitle":"disclaimer: all intellectual property and techniques described in this article have been previously disclosed in US Patents US10014002B2…","__typename":"PreviewContent"},"$Post:210532383785.postResponses.responsesConnection({\"paging\":{\"limit\":10}})":{"pagingInfo":null,"stream":[],"__typename":"StreamConnection"}}</script><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/manifest.js"></script><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/vendorsmain.js"></script><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/main.js"></script><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/vendorsscreen.js"></script>
<script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/screen_002.js"></script>
<script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/screen.js"></script>
<script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/screen_004.js"></script>
<script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/screen_003.js"></script><script>window.main();</script><script src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/p.js" async="" id="parsely-cf"></script><iframe src="Audio%20AI%20%20isolating%20vocals%20from%20stereo%20music%20using%20Convolutional%20Neural%20Networks_files/console.html" id="vimvixen-console-frame" class="vimvixen-console-frame"></iframe></body></html>